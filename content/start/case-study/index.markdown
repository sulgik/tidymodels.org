---
title: "ì˜ˆì¸¡ ëª¨ë¸ë§ ì‚¬ë¡€ ì—°êµ¬"
weight: 5
tags: [parsnip, recipes, rsample, workflows, tune]
categories: [model fitting, tuning]
description: | 
  Best Practice ë¥¼ ì´ìš©í•˜ì—¬ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œí•˜ê¸°.
---






## ë“¤ì–´ê°€ê¸° {#intro}

ì•ì„  [_ì‹œì‘í•˜ê¸°ì„¹ì…˜_](/start/)ì˜ ë„¤ ì¥ì—ì„œëŠ” ëª¨ë¸ë§ì— ì§‘ì¤‘í–ˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¤„ê¸°ì—ì„œ ëª¨ë¸ì‘ì—…í•  ë•Œ í•„ìš”í•œ tidymodles ìƒíƒœê³„ì˜ í•µì‹¬ íŒ¨í‚¤ì§€ë“¤ê³¼ í•µì‹¬ í•¨ìˆ˜ë“¤ì„ ì†Œê°œí–ˆì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸° ì‚¬ë¡€ ì—°êµ¬ì—ì„œ ì• ì¥ë“¤ì—ì„œ ë°°ìš´ ëª¨ë“  ê²ƒë“¤ ì‚¬ìš©í•˜ì—¬ í˜¸í…” ìˆ™ë°•ì— ê´€í•œ ë°ì´í„°ë¡œ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³¼ ê²ƒì…ë‹ˆë‹¤.


<img src="img/hotel.jpg" width="90%" />


ì´ ì¥ì— ìˆëŠ” ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´,  ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì„ ì¸ìŠ¤í†¨í•´ì•¼ í•©ë‹ˆë‹¤: glmnet, ranger, readr, tidymodels, and vip.


```r
library(tidymodels)  

# Helper packages
library(readr)       # for importing data
library(vip)         # for variable importance plots
```

{{< test-drive url="https://rstudio.cloud/project/2674862" >}}

## í˜¸í…” ë¶€í‚¹ ë°ì´í„° {#data}

[Antonio, Almeida, and Nunes (2019)](https://doi.org/10.1016/j.dib.2018.11.126) ì— ìˆëŠ” í˜¸í…” ë¶€í‚¹ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬í–‰ê°ë“¤ì´ ì–´ëŠ í˜¸í…”ì—ì„œ ë¬µì—ˆëŠ”ì§€, ê°€ê²©ì€ ì–¼ë§ˆë‚˜ì˜€ëŠ”ì§€ ë“±ì— ê´€í•œ íŠ¹ì§•ë“¤ì— ê¸°ë°˜í•˜ì—¬ ì–´ë–¤ í˜¸í…”ì—ì„œ ì–´ë¦°ì´ì™€ ì•„ê¸°ê°€ ë¬µì„ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì˜ˆì¸¡í•´ë´…ì‹œë‹¤. 

ì´ ë°ì´í„°ì…‹ì€ [`#TidyTuesday`](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-02-11) ë°ì´í„°ì…‹ì´ê³  ë³€ìˆ˜ì˜ ì •ë³´ëŠ” [data dictionary](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-02-11#data-dictionary)ì— ë‹´ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤. ì‚¬ë¡€ ì—°êµ¬ë¥¼ ìœ„í•´ [í•´ë‹¹ ë°ì´í„°ì…‹ì˜ í¸ì§‘ë²„ì „ ë°ì´í„°ì…‹](https://gist.github.com/topepo/05a74916c343e57a71c51d6bc32a21ce)ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. í˜¸í…” ë°ì´í„°ë¥¼ R ë¡œ ë¶ˆëŸ¬ì™€ ë´…ì‹œë‹¤. ìš°ë¦¬ CSV ë°ì´í„°ê°€ ìœ„ì¹˜í•œ url ("<https://tidymodels.org/start/case-study/hotels.csv>") ì„ [`readr::read_csv()`](https://readr.tidyverse.org/reference/read_delim.html) ì— ì•Œë ¤ì¤ë‹ˆë‹¤:


```r
library(tidymodels)
library(readr)

hotels <- 
  read_csv('https://tidymodels.org/start/case-study/hotels.csv') %>%
  mutate_if(is.character, as.factor) 

dim(hotels)
#> [1] 50000    23
```

ì› ë…¼ë¬¸ì—ì„œ [ì €ìë“¤](https://doi.org/10.1016/j.dib.2018.11.126)ì€ ë§ì€ ë³€ìˆ˜(such as number of adults/children, room type, meals bought, country of origin of the guests, and so forth)ì˜ ë¶„í¬ë“¤ì´ ì·¨ì†Œê±´ê³¼ ì·¨ì†Œí•˜ì§€ ì•Šì€ ê±´ ì‚¬ì´ì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤ê³  ê²½ê³ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì •ë³´ ëŒ€ë¶€ë¶„ì´ ìˆ™ë°•ê°ì´ ì²´í¬ì¸ í•  ë•Œ ëª¨ì•„ì§€ê³ , ë”°ë¼ì„œ ì·¨ì†Œëœ ë¶€í‚¹ì€ ì·¨ì†Œë˜ì§€ ì•Šì€ ë¶€í‚¹ë³´ë‹¤ ê²°ì¸¡ ë°ì´í„°ê°€ ë” ë§ì•„ì„œ, ë°ì´í„°ê°€ ê²°ì¸¡ë˜ì§€ ì•Šì•˜ì„ ë•Œ ë‹¤ë¥¸ íŠ¹ì§•ë“¤ì„ ê°€ì§ˆ ê²ƒì´ê¸° ë•Œë¬¸ì—, ì´ëŸ¬í•œ ì°¨ì´ë¥¼ ì˜ˆìƒí•˜ëŠ” ê²ƒì€ í•©ë¦¬ì ì…ë‹ˆë‹¤. ì´ë¥¼ ê°ì•ˆí•˜ì—¬ ì´ ë°ì´í„°ì—ì„œ ë¶€í‚¹ì„ ìº”ìŠ¬í•œ ì†ë‹˜ê³¼ í•˜ì§€ ì•Šì€ ì†ë‹˜ ì‚¬ì´ì— ì˜ë¯¸ìˆëŠ” ì°¨ì´ë¥¼ ë°œê²¬í•˜ê¸° ì‰½ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë°ì´í„°ë¥¼ ì´ë¯¸ í•„í„°ë§í•´ì„œ ì·¨ì†Œí•˜ì§€ ì•Šì€ ë¶€í‚¹ë§Œ í¬í•¨í–ˆê¸° ë•Œë¬¸ì— _í˜¸í…”ìˆ™ë°•_ ë§Œ ë¶„ì„í•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤. 


```r
glimpse(hotels)
#> Rows: 50,000
#> Columns: 23
#> $ hotel                          <fct> City_Hotel, City_Hotel, Resort_Hotel, Râ€¦
#> $ lead_time                      <dbl> 217, 2, 95, 143, 136, 67, 47, 56, 80, 6â€¦
#> $ stays_in_weekend_nights        <dbl> 1, 0, 2, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1, â€¦
#> $ stays_in_week_nights           <dbl> 3, 1, 5, 6, 4, 2, 2, 3, 4, 2, 2, 1, 2, â€¦
#> $ adults                         <dbl> 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, â€¦
#> $ children                       <fct> none, none, none, none, none, none, chiâ€¦
#> $ meal                           <fct> BB, BB, BB, HB, HB, SC, BB, BB, BB, BB,â€¦
#> $ country                        <fct> DEU, PRT, GBR, ROU, PRT, GBR, ESP, ESP,â€¦
#> $ market_segment                 <fct> Offline_TA/TO, Direct, Online_TA, Onlinâ€¦
#> $ distribution_channel           <fct> TA/TO, Direct, TA/TO, TA/TO, Direct, TAâ€¦
#> $ is_repeated_guest              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦
#> $ previous_cancellations         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦
#> $ previous_bookings_not_canceled <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦
#> $ reserved_room_type             <fct> A, D, A, A, F, A, C, B, D, A, A, D, A, â€¦
#> $ assigned_room_type             <fct> A, K, A, A, F, A, C, A, D, A, D, D, A, â€¦
#> $ booking_changes                <dbl> 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦
#> $ deposit_type                   <fct> No_Deposit, No_Deposit, No_Deposit, No_â€¦
#> $ days_in_waiting_list           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦
#> $ customer_type                  <fct> Transient-Party, Transient, Transient, â€¦
#> $ average_daily_rate             <dbl> 80.75, 170.00, 8.00, 81.00, 157.60, 49.â€¦
#> $ required_car_parking_spaces    <fct> none, none, none, none, none, none, nonâ€¦
#> $ total_of_special_requests      <dbl> 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 0, 1, 0, â€¦
#> $ arrival_date                   <date> 2016-09-01, 2017-08-25, 2016-11-19, 20â€¦
```

ìš°ë¦¬ëŠ” ì‹¤ì œ ìˆ™ë°•ì´ ì–´ë¦°ì´ë“¤ì´ë‚˜ ì•„ê¸°ë¥¼ í¬í•¨í–ˆëŠ”ì§€, ì•„ë‹Œì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³¼ ê²ƒì…ë‹ˆë‹¤. ë°˜ì‘ ë³€ìˆ˜ `children` ì€ ìˆ˜ì¤€ì´ ë‘ê°œì¸ íŒ©í„°í˜• ë³€ìˆ˜ì…ë‹ˆë‹¤:


```r
hotels %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))
#> # A tibble: 2 Ã— 3
#>   children     n   prop
#>   <fct>    <int>  <dbl>
#> 1 children  4038 0.0808
#> 2 none     45962 0.919
```

ì•„ì´ë“¤ì€ ì˜ˆì•½ ìˆ˜ ì¤‘ ë‹¨ì§€ 8.1% ì—ë§Œ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì€ ë¶„ì„ì„ ì¢Œì§€ìš°ì§€ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [recipes](/find/recipes/) ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ [themis](https://tidymodels.github.io/themis/)ì™€ ê°™ì€ ë” ì „ë¬¸í™”ëœ íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ì´ìŠˆì™€ ì‹¸ìš¸ ìˆ˜ ìˆì§€ë§Œ, ì•„ë˜ì— ë‚˜ì™€ìˆëŠ” ë°©ë²•ë“¤ì€ ë°ì´í„°ë¥¼ ìˆëŠ” ê·¸ëŒ€ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

## ë°ì´í„° ìª¼ê°œê¸°ì™€ ë¦¬ìƒ˜í”Œë§ {#data-split}

ë°ì´í„° ìª¼ê°œê¸° ì „ëµìœ¼ë¡œ ìˆ™ë°•ë°ì´í„°ì˜ 25% ë¥¼ í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë”°ë¡œ ë–¼ì–´ ë´…ì‹œë‹¤. 
of the stays to the test set. As in our [*resampling ìœ¼ë¡œ ëª¨ë¸ í‰ê°€í•˜ê¸°*](/start/resampling/#data-split) ì¥ì—ì„œì™€ ê°™ì´ ë°˜ì‘ë³€ìˆ˜ `children` ì´ ê½¤ ë¶ˆê· í˜•ì¸ê²ƒì„ ì•Œê¸° ë•Œë¬¸ì—, ì¸µí™” ëœë¤ ìƒ˜í”Œì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤:  


```r
set.seed(123)
splits      <- initial_split(hotels, strata = children)

hotel_other <- training(splits)
hotel_test  <- testing(splits)

# training set proportions by children
hotel_other %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))
#> # A tibble: 2 Ã— 3
#>   children     n   prop
#>   <fct>    <int>  <dbl>
#> 1 children  3027 0.0807
#> 2 none     34473 0.919

# test set proportions by children
hotel_test  %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))
#> # A tibble: 2 Ã— 3
#>   children     n   prop
#>   <fct>    <int>  <dbl>
#> 1 children  1011 0.0809
#> 2 none     11489 0.919
```

ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ì£¼ìš” ë¦¬ìƒ˜í”Œë§ ë°©ë²•ìœ¼ë¡œ [`rsample::vfold_cv()`](https://tidymodels.github.io/rsample/reference/vfold_cv.html) ì„ ì‚¬ìš©í•œ 10-fold cross-validation ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ íŠ¸ë ˆì´ë‹ì…‹ ( _ë¶„ì„_ ê³¼ _í‰ê°€_ ì…‹ìœ¼ë¡œ ë” ìª¼ê°¬) 10 ê°œì˜ ë‹¤ë¥¸ ë¦¬ìƒ˜í”Œë“¤ì„ ìƒì„±í•˜ê³  10 ê°œì˜ ë‹¤ë¥¸ ì„±ëŠ¥ì§€í‘œë“¤ì„ ìƒì„±í•œ ë’¤ ì·¨í•©í•œë‹¤.

ì´ë²ˆ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ì— ëŒ€í•´, ì—¬ëŸ¬ë²ˆ ë¦¬ìƒ˜í”Œë§ í•˜ëŠ” ê²ƒ ëŒ€ì‹ , _validation set_ ì´ë¼ê³  ë¶€ë¥´ëŠ” ë¦¬ìƒ˜í”Œ í•˜ë‚˜ë§Œ ìƒì„±í•´ë´…ì‹œë‹¤. tidymodels ì—ì„œ validation set ì€ ë¦¬ìƒ˜í”Œë§ ì¼ë°˜ë³µìœ¼ë¡œ ì·¨ê¸‰ë©ë‹ˆë‹¤. `hotel_other` ë¼ê³  ë¶€ë¥´ëŠ” í…ŒìŠ¤íŒ… ì‚¬ìš©ë˜ì§€ ì•Šì€ 37,500 ê°œì˜ ìˆ™ë°•ìœ¼ë¡œ ë¶€í„° split ì´ ë  ê²ƒì…ë‹ˆë‹¤. ì´ split ì€ ë‘ ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤:

+ _validation set_ ì´ë¼ê³  ë¶€ë¥´ëŠ”, ì„±ëŠ¥ì¸¡ì • ëª©ì ìœ¼ë¡œ ë”°ë¡œ ë–¼ì–´ë‚¸ ì…‹

+ _training set_ ì´ë¼ê³  ë¶€ë¥´ëŠ”, ëª¨ë¸ ì í•©í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë‚¨ì€ ë°ì´í„°ì…‹. 

<img src="img/validation-split.svg" width="50%" style="display: block; margin: auto;" />

`validation_split()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 20% of the `hotel_other` ìˆ™ë°•ì˜ 20% ë¥¼  _validation set_ ì— 30,000 ìˆ™ë°•ì€ _training set_ ì— í• ë‹¹í•  ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ìš°ë¦¬ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œê°€ 7,500 ê°œì˜ í˜¸í…” ìˆ™ë°• ë°ì´í„°ì…‹ìœ¼ë¡œ ê³„ì‚°ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ê½¤ í° ê·œëª¨ì—¬ì„œ, ì´ëŸ¬í•œ ë°ì´í„° ì–‘ì€ ê° ëª¨ë¸ì´ ë¦¬ìƒ˜í”Œë§ ì¼ë°˜ë³µìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ì— ëŒ€í•œ ë¯¿ì„ë§Œí•œ ì§€í‘œê°€ ë˜ê¸° ì¶©ë¶„í•œ precision ì„ ì œê³µí•  ê²ƒì…ë‹ˆë‹¤.


```r
set.seed(234)
val_set <- validation_split(hotel_other, 
                            strata = children, 
                            prop = 0.80)
val_set
#> # Validation Set Split (0.8/0.2)  using stratification 
#> # A tibble: 1 Ã— 2
#>   splits               id        
#>   <list>               <chr>     
#> 1 <split [30000/7500]> validation
```

ì´ í•¨ìˆ˜ëŠ” `initial_split()` ê³¼ ê°™ì´ `strata` ì¸ìˆ˜ë¥¼ ê°–ëŠ”ë°, ì¸µí™” ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” ìƒˆë¡œìš´ validation ê³¼ training set ì´ ì•„ì´ë“¤ì´ ìˆê³  ì—†ëŠ” ìˆ™ë°•ì˜ ë¹„ìœ¨ì´ ì› `hotel_other` ë¹„ìœ¨ê³¼ ë¹„êµí•˜ì—¬ ëŒ€ëµ ê°™ì€ ë¹„ìœ¨ì´ ë  ê²ƒì´ë¼ëŠ” ì ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

## ì²« ëª¨ë¸: penalized logistic regression {#first-model}

ìš°ë¦¬ì˜ ë°˜ì‘ë³€ìˆ˜ `children` ì€ ë²”ì£¼í˜•ì´ê¸° ë•Œë¬¸ì—, ë¡œì§€ìŠ¤í‹± íšŒê·€ê°€ ì‹œì‘í•˜ê¸° ì¢‹ì€ ì²«ë²ˆì§¸ ëª¨ë¸ì´ ë©ë‹ˆë‹¤. íŠ¸ë ˆì´ë‹ ë™ì•ˆ feature selection ì„ ìˆ˜í–‰í•  ëª¨ë¸ì„ ì‚¬ìš©í•´ë´…ì‹œë‹¤. [glmnet](https://cran.r-project.org/web/packages/glmnet/index.html) R íŒ¨í‚¤ì§€ëŠ” penalized maximum likelihood ë¥¼ í†µí•´ ì¼ë°˜í™” ì„ í˜• ëª¨í˜•ì„ ì í•©í•©ë‹ˆë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ ê¸°ìš¸ê¸° íŒŒë¼ë¯¸í„° ì¶”ì •ë°©ë²•ì€ _penalty_ ë¥¼ í”„ë¡œì„¸ìŠ¤ì— ì‚¬ìš©í•´ì„œ ëœ ê´€ë ¨ëœ ì„¤ëª…ë³€ìˆ˜ë“¤ì„ 0 ê°’ìœ¼ë¡œ ë³´ëƒ…ë‹ˆë‹¤. glmnet penalization ë°©ë²•ë“¤ ì¤‘ í•˜ë‚˜ì¸, [lasso method](https://en.wikipedia.org/wiki/Lasso_(statistics)) ì€ ì¶©ë¶„íˆ í° penalty ê°€ ì‚¬ìš©ë˜ë©´ ì„¤ëª…ë³€ìˆ˜ ê¸°ìš¸ê¸°ë¥¼ ì‹¤ì œ 0 ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

### ëª¨ë¸ ë§Œë“¤ê¸°

feature selection penalty ë¥¼ ì‚¬ìš©í•˜ëŠ” penalized ë¡œì§€ìŠ¤í‹± íšŒêµ¬ ëª¨ë¸ì„ specify í•˜ê¸° ìœ„í•´ parsnip íŒ¨í‚¤ì§€ë¥¼ [glmnet engine](/find/parsnip/) ê³¼ ì‚¬ìš©í•´ ë´…ì‹œë‹¤:  


```r
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

ì—¬ê¸°ì„œ `tune()` ì„ ì´ìš©í•´ì„œ íŠœë‹í•  `penalty` ì¸ìˆ˜ë¥¼ ì§€ê¸ˆì€ placeholder ë¡œ ì„¤ì •í•  ê²ƒì…ë‹ˆë‹¤. ì´ ì¸ìˆ˜ëŠ” ìš°ë¦¬ ë°ì´í„°ë¡œ ì˜ˆì¸¡ì„ í•˜ê¸° ìœ„í•œ ê°€ì¥ ì¢‹ì€ ê°’ì„ ì°¾ì•„ì•¼ ì°¾ëŠ” [íŠœë‹](/start/tuning/)í•  ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. `mixture` ë¥¼ 1 ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì€ glmnet ëª¨ë¸ì´ ì ì¬ì ìœ¼ë¡œ ê´€ê³„ì—†ëŠ” ì˜ˆì¸¡ì„ ì œê±°í•˜ê³  ë‹¨ìˆœí•œ ëª¨ë¸ì„ ì„ íƒí•  ê²ƒì´ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 

### ë ˆì‹œí”¼ ìƒì„±í•˜ê¸° 

[recipe](/start/recipes/) ë¥¼ ìƒì„±í•˜ì—¬ ì´ ëª¨ë¸ì„ ìœ„í•´ í˜¸í…”ìˆ™ë°• ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ì „ì²˜ë¦¬ ê³¼ì •ì„ ì •ì˜í•´ ë´…ì‹œë‹¤. ë„ì°© ë‚ ì§œì— ê´€ë ¨ëœ ì¤‘ìš”í•œ êµ¬ì„±ìš”ì†Œë“¤ì„ ë°˜ì˜í•˜ëŠ” ë°ì´í„° ê¸°ë°˜ ì„¤ëª…ë³€ìˆ˜ ì…‹ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ì˜ë¯¸ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ë¯¸ ì•ì—ì„œ [ì—¬ëŸ¬ recipe step](/start/recipes/#features)ì„ ì†Œê°œí•˜ì—¬ ë‚ ì§œë¡œ ë¶€í„° í”¼ì³ë“¤ì„ ìƒì„±í•´ë³´ì•˜ìŠµë‹ˆë‹¤:

+ `step_date()` ì€ ì—°ë„, ì›”, ìš”ì¼ ì„¤ëª…ë³€ìˆ˜ë¥¼ ìƒì„±.

+ `step_holiday()` ì€ íŠ¹ë³„í•œ holiday ë¥¼ ê°€ë¦¬í‚¤ëŠ” ë³€ìˆ˜ ì§‘í•©ì„ ìƒì„±. ì´ í˜¸í…”ì´ ì–´ë””ì— ìœ„ì¹˜í•´ ìˆëŠ”ì§€ ì•Œì§€ ëª»í•´ë„, ëŒ€ë¶€ë¶„ ìˆ™ë°•ì˜ origin ì„ ìœ„í•œ êµ­ê°€ë“¤ì´ ìœ ëŸ½ì— ê¸°ë°˜í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì€ ì•Œê³  ìˆìŠµë‹ˆë‹¤.

+ `step_rm()` ì€ ë³€ìˆ˜ë“¤ì„ ì œê±°; ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì› ë‚ ì§œ ë³€ìˆ˜ë¥¼ ëª¨ë¸ì—ì„œ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•Šì•„ì„œ, ì´ë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ê²ƒì´ë‹¤.

ì¶”ê°€ì ìœ¼ë¡œ ëª¨ë“  ë²”ì£¼í˜• ì„¤ëª…ë³€ìˆ˜ (ì˜ˆ, `distribution_channel`, `hotel`, ...) ë“¤ì€ ë”ë¯¸ ë³€ìˆ˜ë“¤ë¡œ ë°”ë€” ê²ƒì´ê³ , ëª¨ë“  ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì€ centered ë˜ê³  scaled ë  ê²ƒì´ë‹¤.

+ `step_dummy()` ëŠ” ë¬¸ìì™€ íŒ©í„°í˜• (ì¦‰, ëª…ëª©í˜• ë³€ìˆ˜ë“¤) ì„ ì› ë°ì´í„°ì˜ ìˆ˜ì¤€ë“¤ì„ ìœ„í•œ í•˜ë‚˜ ì´ìƒì˜ ìˆ˜ì¹˜í˜• binary model terms ìœ¼ë¡œ ë³€í™˜.

+ `step_zv()` ì€ í•˜ë‚˜ì˜ ìœ ì¼í•œ ê°’ì„ í•˜ë‚˜ë§Œ í¬í•¨(ì˜ˆ, ëª¨ë‘ 0)í•˜ëŠ” indicator ë³€ìˆ˜ë“¤ì„ ì œê±°. penalized models ì—ì„œ ì„¤ëª…ë³€ìˆ˜ëŠ” center ë˜ê³  scale ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì´ ìŠ¤í…ì€ ì¤‘ìš”í•©ë‹ˆë‹¤.

+ `step_normalize()` ëŠ” ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì„ centering í•˜ê³  scaling í•¨.

ì´ ëª¨ë“  ìŠ¤í…ì˜ penalized logistic regression ëª¨ë¸ì˜ ë ˆì‹œí”¼ë¡œ ë¬¶ìœ¼ë©´: 


```r
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- 
  recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>% 
  step_rm(arrival_date) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```


### ì›Œí¬í”Œë¡œ ìƒì„±

[*ë ˆì‹œí”¼ë¡œ ì „ì²˜ë¦¬í•˜ê¸°*](/start/recipes/#fit-workflow)ì—ì„œì™€ ê°™ì´, ëª¨ë¸ì™€ ë ˆì‹œí”¼ë¥¼ í•˜ë‚˜ì˜ `workflow()` ê°ì²´ë¡œ ë²ˆë“¤í•˜ì—¬ R ê°ì²´ ê´€ë¦¬ë¥¼ ë” ì‰½ê²Œ í•´ ë´…ì‹œë‹¤:


```r
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)
```

### íŠœë‹ì„ ìœ„í•œ ê·¸ë¦¬ë“œ ìƒì„±

ì´ ëª¨ë¸ì„ ì í•©í•˜ê¸° ì „ì—, íŠœë‹í•  `penalty` ê°’ì˜ ê·¸ë¦¬ë“œë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. [*ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹í•˜ê¸°*](/start/tuning/) ì¥ì—ì„œ [`dials::grid_regular()`](start/tuning/#tune-grid)ì„ ì‚¬ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë‘ê°œì˜ ì¡°í•©ì— ê¸°ë°˜í•˜ì—¬ expanded ê·¸ë¦¬ë“œë¥¼ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ íŠœë‹í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ í•˜ë‚˜ì´ë¯€ë¡œ, 30ê°œ í›„ë³´ ê°’ë“¤ì„ ê°€ì§„ í•˜ë‚˜ì˜ ì—´ í‹°ë¸”ì„ ìˆ˜ë™ìœ¼ë¡œ ì´ìš©í•˜ì—¬ ê·¸ë¦¬ë“œë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```r
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_reg_grid %>% top_n(-5) # lowest penalty values
#> Selecting by penalty
#> # A tibble: 5 Ã— 1
#>    penalty
#>      <dbl>
#> 1 0.0001  
#> 2 0.000127
#> 3 0.000161
#> 4 0.000204
#> 5 0.000259
lr_reg_grid %>% top_n(5)  # highest penalty values
#> Selecting by penalty
#> # A tibble: 5 Ã— 1
#>   penalty
#>     <dbl>
#> 1  0.0386
#> 2  0.0489
#> 3  0.0621
#> 4  0.0788
#> 5  0.1
```

### ëª¨ë¸ í›ˆë ¨ê³¼ íŠœë‹í•˜ê¸°

`tune::tune_grid()` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ 30 ê°œì˜ penalized logistic regression models ì„ í›ˆë ¨ì‹œì¼œ ë´…ì‹œë‹¤. validation set ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ìˆ˜ ìˆëŠ”ë° (`control_grid()` í˜¸ì¶œ ì‚¬ìš©) ì´ë ‡ê²Œ í•˜ë©´, ì§„ë‹¨ì •ë³´ê°€ ëª¨ë¸ ì í•© ì´í›„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²¤íŠ¸ threshold ì˜ continum ì„ í†µíŠ¼ ëª¨ë¸ ì„±ëŠ¥ì„ ì •ëŸ‰í™”í•˜ëŠ”ë° Area under ROC ì»¤ë¸Œë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤ (event rate&mdash;ì•„ì´ë“¤ì„ í¬í•¨í•œ ìˆ™ë°•ë¹„ìœ¨&mdash ì´ ë°ì´í„°ì—ì„œ ë§¤ìš° ë‚®ìŒì„ ê¸°ì–µí•˜ì„¸ìš”).


```r
lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

area under the ROC ì»¤ë¸Œë¥¼ penalty ê°’ë“¤ ë²”ìœ„ì— ìƒëŒ€í•˜ì—¬ plotting í•˜ë©´ validation set ì§€í‘œë“¤ì„ ì‹œê°í™” í•˜ëŠ” ê²ƒì´ ë” ì‰¬ìš¸ ê²ƒì…ë‹ˆë‹¤:


```r
lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())

lr_plot 
```

<img src="figs/logistic-results-1.svg" width="576" />

ì´ í”Œë¡¯ì€ ëª¨ë¸ ì„±ëŠ¥ì´ ë” ì‘ì€ penalty ê°’ë“¤ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ë” ì¢‹ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” ì„¤ëª…ë³€ìˆ˜ ëŒ€ë¶€ë¶„ì´ ëª¨ë¸ì— ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. ROC ì»¤ë¸Œê°€ ë†’ì€ penalty ê°’ì—ì„œ ê°€íŒŒë¥´ê²Œ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì¶©ë¶„íˆ í° penalty ëŠ” ëª¨ë¸ì—ì„œ _ëª¨ë“ _ ì„¤ëª…ë³€ìˆ˜ë“¤ì„ ì œê±°í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ë°œìƒí•©ë‹ˆë‹¤. ì˜ˆì¸¡ì •í™•ë„ê°€ ì„¤ëª…ë³€ìˆ˜ê°€ ì—†ëŠ” ëª¨ë¸ì—ì„œ ê¸‰ê°í•˜ëŠ” ê²ƒì€ ë†€ë¼ìš´ ì¼ì´ ì•„ë‹™ë‹ˆë‹¤ (0.50 ROC AUC ê°’ì€ ëª¨ë¸ì´ ë§ëŠ” í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•  ë•Œ ìš°ì—°íˆ í•˜ëŠ” ê²ƒê³¼ ì„±ëŠ¥ì´ ê°™ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì„¸ìš”).

ìš°ë¦¬ ëª¨ë¸ ì„±ëŠ¥ì€ ë” ì‘ì€ í˜ë„í‹° ê°’ì—ì„œ í‰í‰í•œ ê²ƒ ì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. ë”°ë¼ì„œ `roc_auc` í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ë©´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ "ê°€ì¥ì¢‹ì€" ê°’ì— ì—¬ëŸ¬ ì˜µì…˜ë“¤ì´ ìˆë‹¤ê³  ê²°ë¡ ë‚´ë¦¬ê²Œ ë©ë‹ˆë‹¤: 


```r
top_models <-
  lr_res %>% 
  show_best("roc_auc", n = 15) %>% 
  arrange(penalty) 
top_models
#> # A tibble: 15 Ã— 7
#>     penalty .metric .estimator  mean     n std_err .config              
#>       <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                
#>  1 0.000127 roc_auc binary     0.872     1      NA Preprocessor1_Model02
#>  2 0.000161 roc_auc binary     0.872     1      NA Preprocessor1_Model03
#>  3 0.000204 roc_auc binary     0.873     1      NA Preprocessor1_Model04
#>  4 0.000259 roc_auc binary     0.873     1      NA Preprocessor1_Model05
#>  5 0.000329 roc_auc binary     0.874     1      NA Preprocessor1_Model06
#>  6 0.000418 roc_auc binary     0.874     1      NA Preprocessor1_Model07
#>  7 0.000530 roc_auc binary     0.875     1      NA Preprocessor1_Model08
#>  8 0.000672 roc_auc binary     0.875     1      NA Preprocessor1_Model09
#>  9 0.000853 roc_auc binary     0.876     1      NA Preprocessor1_Model10
#> 10 0.00108  roc_auc binary     0.876     1      NA Preprocessor1_Model11
#> 11 0.00137  roc_auc binary     0.876     1      NA Preprocessor1_Model12
#> 12 0.00174  roc_auc binary     0.876     1      NA Preprocessor1_Model13
#> 13 0.00221  roc_auc binary     0.876     1      NA Preprocessor1_Model14
#> 14 0.00281  roc_auc binary     0.875     1      NA Preprocessor1_Model15
#> 15 0.00356  roc_auc binary     0.873     1      NA Preprocessor1_Model16
```



ì´ í‹°ë¸”ì˜ ëª¨ë“  í›„ë³´ëª¨ë¸ì€ ì•„ë˜ í–‰ì˜ ëª¨ë¸ë³´ë‹¤ ë” ë§ì€ ì„¤ëª…ë³€ìˆ˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤. `select_best()` ë¥¼ í•˜ë©´ ì ì„ ë³´ë‹¤ ë‚®ì€ ê°’ì— ë³´ì´ëŠ” 0.00137 í˜ë„í‹° ê°’ì„ ê°€ì§„ í›„ë³´ ëª¨ë¸ 11 ë¥¼ ë°˜í™˜í•  ê²ƒì…ë‹ˆë‹¤.

<img src="figs/lr-plot-lines-1.svg" width="576" />

í•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” penalty value ë¥¼ ëª¨ë¸ì„±ëŠ¥ì´ ë–¨ì–´ì§€ê¸° ì‹œì‘í•˜ëŠ” ê³³ ê°€ê¹Œì´ x-ì¶•ì„ ë”°ë¼ ë” ë¨¼ ê°’ìœ¼ë¡œ ì„ íƒí•˜ê¸¸ ì›í•  ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 0.00174 penalty ê°’ì„ ê°€ì§„ í›„ë³´ ëª¨ë¸ 12 ì€ ìˆ˜ì¹˜ì ìœ¼ë¡œ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ê³¼ ê°™ì€ ì„±ëŠ¥ì„ ê°€ì§€ì§€ë§Œ ì„¤ëª…ë³€ìˆ˜ë¥¼ ë” ë§ì´ ì œê±°í•©ë‹ˆë‹¤. ì´ í˜ë„í‹° ê°’ì€ ìœ„ì—ì„œ ì‹¤ì„ ìœ¼ë¡œ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ëœ ê´€ë ¨ëœ ì„¤ëª…ë³€ìˆ˜ê°€ í¬í•¨ë˜ì§€ ì•Šì„ ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤. ì„±ëŠ¥ì´ ëŒ€ëµ ë¹„ìŠ·í•˜ë‹¤ë©´ penalty ê°’ì´ í° ê²ƒì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 

ì´ ê°’ì„ ì„ íƒí•˜ê³  validation set ROC ì»¤ë¸Œë¥¼ ì‹œê°í™”í•´ë´…ì‹œë‹¤:

```r
lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(12)
lr_best
#> # A tibble: 1 Ã— 7
#>   penalty .metric .estimator  mean     n std_err .config              
#>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                
#> 1 0.00137 roc_auc binary     0.876     1      NA Preprocessor1_Model12
```



```r
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```

<img src="figs/logistic-roc-curve-1.svg" width="672" />

ì´ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì´ ìƒì„±í•œ ì„±ëŠ¥ ìˆ˜ì¤€ì€ ì¢‹ì§€ë§Œ, groundbreaking ì€ ì•„ë‹™ë‹ˆë‹¤. ì•„ë§ˆ ì˜ˆì¸¡ ë“±ì‹ì˜ ì„ í˜• ì†ì„±ì´ ì´ ë°ì´í„°ì…‹ì„ ì œì•½í•˜ê³  ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ, tree-ê¸°ë°˜ ì•™ìƒë¸” ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ìƒì„±ëœ highly ë¹„ì„ í˜• ëª¨ë¸ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ë‘ë²ˆì§¸ ëª¨ë¸: tree-based ensemble {#second-model}

íš¨ê³¼ì ì´ë©´ì„œ low-maintenance ëª¨ë¸ë§ ê¸°ë²•ì€ _ëœë¤í¬ë ˆìŠ¤íŠ¸_ ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ [*resampling ìœ¼ë¡œ ëª¨ë¸í‰ê°€í•˜ê¸°*](/start/resampling/) ì¥ì—ì„œ ì‚¬ìš©ë˜ê¸°ë„ í–ˆìŠµë‹ˆë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ë¹„êµí•˜ì—¬, ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì€ ë” ìœ ì—°í•©ë‹ˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê° ìˆ˜ì²œê°œì˜ decision tree ë“¤ë¡œ êµ¬ì„±ëœ _ì•™ìƒë¸”ëª¨ë¸_ ì…ë‹ˆë‹¤. ê° íŠ¸ë¦¬ëŠ” ì•½ê°„ ë‹¤ë¥¸ ë²„ì „ì˜ íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¥¼ ë§Œë‚˜ê²Œ ë˜ê³ , ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ splitting rule sequence ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ê° tree ëŠ” ë¹„ì„ í˜•ì´ê³  treeë“¤ì„ aggregate í•˜ë©´ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¥¼ ë˜í•œ ë¹„ì„ í˜•ì´ ë˜ì§€ë§Œ ë‹¨ì¼ tree ì— ë¹„í•´ ë” robust í•˜ê³  ì•ˆì •ì„±ìˆê²Œ ë©ë‹ˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê°™ì€ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ë“¤ì€ ì „ì²˜ë¦¬ë¥¼ ê±°ì˜ í•„ìš”ë¡œ í•˜ì§€ ì•Šê³  ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì„¤ëª…ë³€ìˆ˜ë“¤(sparse, skewed, continuous, categorical ë“±)ì„ íš¨ê³¼ì ìœ¼ë¡œ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

### ëª¨ë¸ êµ¬ì¶•ê³¼ í•™ìŠµ ì‹œê°„ ê°œì„ 

ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ ê¸°ë³¸ê°’ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ê½¤ ê´œì°®ì€ ê²°ê³¼ë¥¼ ì£¼ê³¤ í•˜ì§€ë§Œ, ì„±ëŠ¥ì´ ê°œì„ ì‹œí‚¬ ê±°ë¼ê³  ìƒê°ë˜ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„° ë‘ê°œë¥¼ íŠœë‹í•´ ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì•ˆíƒ€ê¹ê²Œë„, ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê³  íŠœë‹í•˜ëŠ” ê²ƒì€ computationally expensive í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ íŠœë‹ì— í•„ìš”í•œ ê³„ì‚°ì€ í•™ìŠµ ì‹œê°„ì„ ê°œì„ ì‹œí‚¤ê¸° ìœ„í•´ ì‡±ê²Œ ë³‘ë ¬í™” ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. tune íŒ¨í‚¤ì§€ëŠ” [parallel processing](https://tidymodels.github.io/tune/articles/extras/optimizations.html#parallel-processing)ì„ ëŒ€ì‹ í•´ ì¤„ ìˆ˜ ìˆê³  ì‚¬ìš©ìë“¤ì´ ëª¨ë¸ì„ ì í•©í•  ëª©ì ìœ¼ë¡œ ë©€í‹°ì½”ì–´ë‚˜ separate machines ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. 

í•˜ì§€ë§Œ, ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ validation set ì„ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, ë³‘ë ¬í™”ëŠ” tune íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ëŠ” option ì´ ì•„ë‹™ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ì—ì„œ ì—”ì§„ ìì²´ê°€ ì¢‹ì€ ëŒ€ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤. ranger íŒ¨í‚¤ì§€ëŠ” ê°œë³„ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì„ ë³‘ë ¬ë¡œ ê³„ì‚°í•˜ëŠ” ë¹ŒíŠ¸ì¸ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í•˜ê¸° ìœ„í•´, ìš°ë¦¬ê°€ ì‘ì—…í•´ì•¼ í•˜ëŠ” ì½”ì–´ ìˆ˜ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. parallel íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¼ë§ˆë‚˜ ë³‘ë ¬í™”ë¥¼ í•  ìˆ˜ ìˆëŠ”ì§€ ì´í•´í•˜ê¸° ìœ„í•´ ë‹¹ì‹ ì´ ê°€ì§„ ì»´í“¨í„°ì˜ ì½”ì–´ ìˆ˜ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```r
cores <- parallel::detectCores()
cores
#> [1] 8
```

ìš°ë¦¬ëŠ” 8 ê°œì˜ ì½”ì–´ë¡œ ì‘ì—…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. parsnip `rand_forest()` ëª¨ë¸ì„ ì„¤ì •í•  ë•Œ ranger ì—”ì§„ì—ê²Œ ì´ ì •ë³´ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³‘ë ¬ í”„ë¡œì„¸ì‹±ì„ í•˜ê¸° ìœ„í•´, `num.threads` ì™€ ê°™ì€ ì—”ì§„-specific í•œ ì¸ìˆ˜ë“¤ì„ ë‹¤ìŒê³¼ ê°™ì´ ranger ì— ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```r
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")
```

ì´ ëª¨ë¸ë§ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì˜ ì‘ë™í•˜ì§€ë§Œ, ë°˜ë³µí•˜ì§€ ëª»í•©ë‹ˆë‹¤: ë‹¤ë¥¸ resampling ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´, tune ì´ ìë™ìœ¼ë¡œ ë³‘ë ¬ í”„ë¡œì„¸ì‹±ì„ í•©ë‹ˆë‹¤&mdash; ë³‘ë ¬ í”„ë¡œì„¸ì‹±ì„ ìœ„í•´ ì¼ë°˜ì ìœ¼ë¡œ (ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ í–ˆë“¯ì´) ëª¨ë¸ë§ ì—”ì§„ì— ì˜ì¡´í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì´ ëª¨ë¸ì—ì„œ, ìš°ë¦¬ëŠ” `mtry` ì™€ `min_n` ì¸ìˆ˜ ê°’ë“¤ì„ ìœ„í•œ placeholder ë¡œ `tune()` ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ ì´ë“¤ì´ ìš°ë¦¬ê°€ [íŠœë‹](/start/tuning/)í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤..  

### ë ˆì‹œí”¼ì™€ ì›Œí¬í”Œë¡œ ìƒì„±í•˜ê¸°

penalized logistic regression modelê³¼ ë‹¤ë¥´ê²Œ random forest modelì€ [ë”ë¯¸](https://bookdown.org/max/FES/categorical-trees.html)ë‚˜ ì •ê·œí™”ëœ ì„¤ëª…ë³€ìˆ˜ë“¤ì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  `arrival_date` ë³€ìˆ˜ì— ë‹¤ì‹œí•œë²ˆ í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ì„ í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ì „ê³¼ ê°™ì´ ë‚ ì§œ ì„¤ëª…ë³€ìˆ˜ëŠ” randome forest ê°€ ë°ì´í„°ì—ì„œ ì ì¬ëœ íŒ¨í„´ë“¤ì„ ë„ˆë¬´ ì—´ì‹¬íˆ tease í•˜ì§€ ì•Šë„ë¡ engineered ë˜ì—ˆìŠµë‹ˆë‹¤.


```r
rf_recipe <- 
  recipe(children ~ ., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date) %>% 
  step_rm(arrival_date) 
```

ì´ ë ˆì‹œí”¼ë¥¼ ìš°ë¦¬ parsnip ëª¨ë¸ì— ì¶”ê°€í•´ì„œ í˜¸í…”ìˆ™ë°•ì´ ì–´ë¦°ì´ë‚˜ ì•„ê¸°ë“¤ì„ ê²ŒìŠ¤íŠ¸ë¡œ í¬í•¨í–ˆëŠ”ì§€ ì•„ë‹Œì§€ë¥¼ ëœë¤í¬ë ˆìŠ¤íŠ¸ë¡œ ì˜ˆì¸¡í•˜ëŠ” ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œê°€ ìƒê²¼ìŠµë‹ˆë‹¤. 


```r
rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)
```

### ëª¨ë¸ í›ˆë ¨í•˜ê¸°ì™€ íŠœë‹í•˜ê¸°

parsnip ëª¨ë¸ì„ ì„¤ì •í•  ë•Œ ìš°ë¦¬ëŠ” íŠœë‹í•  ë‘ ê°œì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤:


```r
rf_mod
#> Random Forest Model Specification (classification)
#> 
#> Main Arguments:
#>   mtry = tune()
#>   trees = 1000
#>   min_n = tune()
#> 
#> Engine-Specific Arguments:
#>   num.threads = cores
#> 
#> Computational engine: ranger

# show what will be tuned
rf_mod %>%    
  parameters()  
#> Collection of 2 parameters for tuning
#> 
#>  identifier  type    object
#>        mtry  mtry nparam[?]
#>       min_n min_n nparam[+]
#> 
#> Model parameters needing finalization:
#>    # Randomly Selected Predictors ('mtry')
#> 
#> See `?dials::finalize` or `?dials::update.parameters` for more information.
```

`mtry` í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì˜ì‚¬ê²°ì • ë‚˜ë¬´ì˜ ê° ë…¸ë“œê°€ ë§Œë‚˜ê³  í•™ìŠµí•˜ëŠ” ì„¤ëª…ë³€ìˆ˜ì˜ ìˆ«ìë¥¼ ì„¤ì •í•˜ëŠ”ë°, 1ì—ì„œ ë¶€í„° ì¡´ì¬í•˜ëŠ” í”¼ì³ì˜ ì´ ê°œìˆ˜ê¹Œì§€ì˜ ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤; `mtry` = ê°€ëŠ¥í•œ í”¼ì³ìˆ«ì ì´ë©´ ëª¨ë¸ì€ ë°°ê¹… decision tree ì™€ ê°™ìŠµë‹ˆë‹¤.  `min_n` í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì–´ë–¤ ë…¸ë“œì—ì„œ split í•  ìµœì†Œ `n` ì„ ì„¤ì •í•©ë‹ˆë‹¤.

ìš°ë¦¬ëŠ” íŠœë‹í•  space-filling ë””ìì¸ì„ 25 ê°œì˜ í›„ë³´ ëª¨ë¸ë“¤ê³¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤: 


```r
set.seed(345)
rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
#> i Creating pre-processing data to finalize unknown parameter: mtry
```

ìœ„ì— ì¶œë ¥ë˜ëŠ” *"Creating pre-processing data to finalize unknown parameter: mtry"* ë¼ëŠ” ë©”ì„¸ì§€ëŠ” ë°ì´í„°ì…‹ ì‚¬ì´ì¦ˆì™€ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. `mtry` ëŠ” ë°ì´í„°ì…‹ì—ì„œ ì„¤ëª…ë³€ìˆ˜ì˜ ê°œìˆ˜ì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì—, `tune_grid()` ëŠ” `mtry` ê°€ ë°ì´í„°ë¥¼ ë°›ì„ ë•Œì˜ upper bound ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. 

ì—¬ê¸°ì— 25 ê°œì˜ í›„ë³´ëª¨ë¸ë“¤ ì¤‘ top 5 ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤:


```r
rf_res %>% 
  show_best(metric = "roc_auc")
#> # A tibble: 5 Ã— 8
#>    mtry min_n .metric .estimator  mean     n std_err .config              
#>   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                
#> 1     8     7 roc_auc binary     0.926     1      NA Preprocessor1_Model13
#> 2    12     7 roc_auc binary     0.926     1      NA Preprocessor1_Model01
#> 3    13     4 roc_auc binary     0.925     1      NA Preprocessor1_Model05
#> 4     9    12 roc_auc binary     0.924     1      NA Preprocessor1_Model19
#> 5     6    18 roc_auc binary     0.924     1      NA Preprocessor1_Model24
```

ë°”ë¡œ ì´ ROC AUC ê°’ë“¤ì´ ìš°ë¦¬ê°€ penalized ë¡œì§€ìŠ¤í‹±íšŒê·€ë¥¼ ì‚¬ìš©í•œ, ROC AUC of 0.876 ì˜ ê°’ì„ ì–»ì—ˆë˜ top model ë³´ë‹¤ ë” ì¢‹ì€ ê²ƒì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. 

Plotting the results of the tuning process highlights that both `mtry` (number of predictors at each node) and `min_n` (minimum number of data points required to keep splitting) should be fairly small to optimize performance. However, the range of the y-axis indicates that the model is very robust to the choice of these parameter values &mdash; all but one of the ROC AUC values are greater than 0.90.


```r
autoplot(rf_res)
```

<img src="figs/rf-results-1.svg" width="672" />

Let's select the best model according to the ROC AUC metric. Our final tuning parameter values are:


```r
rf_best <- 
  rf_res %>% 
  select_best(metric = "roc_auc")
rf_best
#> # A tibble: 1 Ã— 3
#>    mtry min_n .config              
#>   <int> <int> <chr>                
#> 1     8     7 Preprocessor1_Model13
```

To calculate the data needed to plot the ROC curve, we use `collect_predictions()`. This is only possible after tuning with `control_grid(save_pred = TRUE)`. In the output, you can see the two columns that hold our class probabilities for predicting hotel stays including and not including children.


```r
rf_res %>% 
  collect_predictions()
#> # A tibble: 187,500 Ã— 8
#>   id         .pred_children .pred_none  .row  mtry min_n children .config       
#>   <chr>               <dbl>      <dbl> <int> <int> <int> <fct>    <chr>         
#> 1 validation         0.152       0.848    13    12     7 none     Preprocessor1â€¦
#> 2 validation         0.0302      0.970    20    12     7 none     Preprocessor1â€¦
#> 3 validation         0.513       0.487    22    12     7 children Preprocessor1â€¦
#> 4 validation         0.0103      0.990    23    12     7 none     Preprocessor1â€¦
#> 5 validation         0.0111      0.989    31    12     7 none     Preprocessor1â€¦
#> # â€¦ with 187,495 more rows
```

To filter the predictions for only our best random forest model, we can use the `parameters` argument and pass it our tibble with the best hyperparameter values from tuning, which we called `rf_best`:


```r
rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Random Forest")
```

Now, we can compare the validation set ROC curves for our top penalized logistic regression model and random forest model: 


```r
bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)
```

<img src="figs/rf-lr-roc-curve-1.svg" width="672" />

The random forest is uniformly better across event probability thresholds. 

## ë§ˆì§€ë§‰ ì í•© {#last-fit}

Our goal was to predict which hotel stays included children and/or babies. The random forest model clearly performed better than the penalized logistic regression model, and would be our best bet for predicting hotel stays with and without children. After selecting our best model and hyperparameter values, our last step is to fit the final model on all the rows of data not originally held out for testing (both the training and the validation sets combined), and then evaluate the model performance one last time with the held-out test set. 

We'll start by building our parsnip model object again from scratch. We take our best hyperparameter values from our random forest model. When we set the engine, we add a new argument: `importance = "impurity"`. This will provide _variable importance_ scores for this last model, which gives some insight into which predictors drive model performance.


```r
# the last model
last_rf_mod <- 
  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit
#> # Resampling results
#> # Manual resampling 
#> # A tibble: 1 Ã— 6
#>   splits                id               .metrics  .notes .predictions .workflow
#>   <list>                <chr>            <list>    <list> <list>       <list>   
#> 1 <split [37500/12500]> train/test split <tibble â€¦ <tibbâ€¦ <tibble [12â€¦ <workfloâ€¦
```

This fitted workflow contains _everything_, including our final metrics based on the test set. So, how did this model do on the test set? Was the validation set a good estimate of future performance? 


```r
last_rf_fit %>% 
  collect_metrics()
#> # A tibble: 2 Ã— 4
#>   .metric  .estimator .estimate .config             
#>   <chr>    <chr>          <dbl> <chr>               
#> 1 accuracy binary         0.946 Preprocessor1_Model1
#> 2 roc_auc  binary         0.923 Preprocessor1_Model1
```

This ROC AUC value is pretty close to what we saw when we tuned the random forest model with the validation set, which is good news. That means that our estimate of how well our model would perform with new data was not too far off from how well our model actually performed with the unseen test data.

We can access those variable importance scores via the `.workflow` column. We first need to [pluck](https://purrr.tidyverse.org/reference/pluck.html) out the first element in the workflow column, then [pull out the fit](https://tidymodels.github.io/workflows/reference/workflow-extractors.html) from the workflow object. Finally, the vip package helps us visualize the variable importance scores for the top 20 features: 


```r
last_rf_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 20)
#> Warning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.
#> Please use `extract_fit_parsnip()` instead.
```

<img src="figs/rf-importance-1.svg" width="672" />

The most important predictors in whether a hotel stay had children or not were the daily cost for the room, the type of room reserved, the type of room that was ultimately assigned, and the time between the creation of the reservation and the arrival date. 

Let's generate our last ROC curve to visualize. Since the event we are predicting is the first level in the `children` factor ("children"), we provide `roc_curve()` with the [relevant class probability](https://tidymodels.github.io/yardstick/reference/roc_curve.html#relevant-level) `.pred_children`:


```r
last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(children, .pred_children) %>% 
  autoplot()
```

<img src="figs/test-set-roc-curve-1.svg" width="672" />

Based on these results, the validation set and test set performance statistics are very close, so we would have pretty high confidence that our random forest model with the selected hyperparameters would perform well when predicting new data.

## ë‹¤ìŒë‹¨ê³„ {#next}

If you've made it to the end of this series of [*Get Started*](/start/) articles, we hope you feel ready to learn more! You now know the core tidymodels packages and how they fit together. After you are comfortable with the basics we introduced in this series, you can [learn how to go farther](/learn/) with tidymodels in your modeling and machine learning projects. 

Here are some more ideas for where to go next:

+ Study up on statistics and modeling with our comprehensive [books](/books/).

+ Dig deeper into the [package documentation sites](/packages/) to find functions that meet your modeling needs. Use the [searchable tables](/find/) to explore what is possible.

+ Keep up with the latest about tidymodels packages at the [tidyverse blog](https://www.tidyverse.org/tags/tidymodels/).

+ Find ways to ask for [help](/help/) and [contribute to tidymodels](/contribute) to help others.

### <center>Happy modeling!</center>

## Session information


```
#> â”€ Session info  ğŸ§  ğŸ§­  ğŸ·   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  hash: cupcake, compass, saxophone
#> 
#>  setting  value
#>  version  R version 4.1.1 (2021-08-10)
#>  os       macOS Big Sur 10.16
#>  system   x86_64, darwin17.0
#>  ui       X11
#>  language (EN)
#>  collate  en_US.UTF-8
#>  ctype    en_US.UTF-8
#>  tz       Asia/Seoul
#>  date     2021-12-27
#>  pandoc   2.11.4 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)
#> 
#> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  package    * version date (UTC) lib source
#>  broom      * 0.7.10  2021-10-31 [1] CRAN (R 4.1.0)
#>  dials      * 0.0.10  2021-09-10 [1] CRAN (R 4.1.0)
#>  dplyr      * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)
#>  ggplot2    * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)
#>  infer      * 1.0.0   2021-08-13 [1] CRAN (R 4.1.0)
#>  parsnip    * 0.1.7   2021-07-21 [1] CRAN (R 4.1.0)
#>  purrr      * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)
#>  ranger       0.13.1  2021-07-14 [1] CRAN (R 4.1.0)
#>  readr      * 2.1.0   2021-11-11 [1] CRAN (R 4.1.0)
#>  recipes    * 0.1.17  2021-09-27 [1] CRAN (R 4.1.0)
#>  rlang        0.4.12  2021-10-18 [1] CRAN (R 4.1.0)
#>  rsample    * 0.1.1   2021-11-08 [1] CRAN (R 4.1.0)
#>  tibble     * 3.1.6   2021-11-07 [1] CRAN (R 4.1.0)
#>  tidymodels * 0.1.4   2021-10-01 [1] CRAN (R 4.1.0)
#>  tune       * 0.1.6   2021-07-21 [1] CRAN (R 4.1.0)
#>  vip        * 0.3.2   2020-12-17 [1] CRAN (R 4.1.0)
#>  workflows  * 0.2.4   2021-10-12 [1] CRAN (R 4.1.0)
#>  yardstick  * 0.0.9   2021-11-22 [1] CRAN (R 4.1.0)
#> 
#>  [1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library
#> 
#> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```
