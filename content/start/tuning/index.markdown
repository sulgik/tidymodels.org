---
title: "ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹í•˜ê¸°"
weight: 4
tags: [rsample, parsnip, tune, dials, workflows, yardstick]
categories: [tuning]
description: | 
  Estimate the best values for hyperparameters that cannot be learned directly during model training.
---






## ë“¤ì–´ê°€ê¸° {#intro}

ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¤‘ ì–´ë–¤ ê²ƒë“¤ì€ ëª¨ë¸ íŠ¸ë ˆì´ë‹ ì¤‘ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶€í„° ì§ì ‘ í•™ìŠµì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°ë¥¼ **í•˜ì´í¼íŒŒë¼ë¯¸í„°** ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì—ì„œ ë‚˜ëˆ„ì–´ì§€ëŠ” ê³³ì—ì„œ ìƒ˜í”Œë˜ëŠ” ì„¤ëª…ë³€ìˆ˜ì˜ ìˆ«ì (tidymodels ì—ì„œ `mtry` ë¡œ ë¶€ë¦„) í˜¹ì€ ë¶€ìŠ¤í‹°ë“œ íŠ¸ë¦¬ ëª¨ë¸ì—ì„œ í•™ìŠµì†ë„(`learn_rate` ë¡œ ë¶€ë¦„) ê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— í¬í•¨ë©ë‹ˆë‹¤. ëª¨ë¸ íŠ¸ë ˆì´ë‹ ì¤‘ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ëŠ”ê²ƒ ëŒ€ì‹ , ë¦¬ìƒ˜í”Œí•œ ë°ì´í„°ì…‹ì— ë§ì€ ëª¨í˜•ì„ í›ˆë ¨í•˜ê³  ì´ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ íƒìƒ‰í•´ì„œ ê°€ì¥ ì¢‹ì€ ê°’ì„ _ì¶”ì •_ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í”„ë¡œì„¸ìŠ¤ë¥¼ **íŠœë‹** ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì˜ˆë¡œ, íŠ¸ë¦¬-ê¸°ë°˜ ëª¨ë¸ì—ì„œ ìª¼ê°œì§ì—ì„œ ìƒ˜í”Œëœ ì„¤ëª…ë³€ìˆ˜ì˜ ìˆ«ì (tidymodels ì—ì„œ `mtry` ë¼ê³  ë¶€ë¦„), í˜¹ì€ ë¶€ìŠ¤í‹°ë“œ íŠ¸ë¦¬ëª¨ë¸ì—ì„œ í•™ìŠµì†ë„(`learning_rate` ì´ë¼ê³  ë¶€ë¦„)ê°€ ìˆìŠµë‹ˆë‹¤. 

ì´ ì¥ì— ìˆëŠ” ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´,  ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì„ ì¸ìŠ¤í†¨í•´ì•¼ í•©ë‹ˆë‹¤: rpart, rpart.plot, tidymodels, and vip.


```r
library(tidymodels)  # for the tune package, along with the rest of tidymodels

# Helper packages
library(rpart.plot)  # for visualizing a decision tree
library(vip)         # for variable importance plots
```

{{< test-drive url="https://rstudio.cloud/project/2674862" >}}

## ì„¸í¬ ì´ë¯¸ì§€ ë°ì´í„°, ê³„ì† {#data}

ì´ì „ì˜ [*ë¦¬ìƒ˜í”Œë§ìœ¼ë¡œ ëª¨ë¸ í‰ê°€í•˜ê¸°*](/start/resampling/) ì¥ì—ì„œ, ì „ë¬¸ê°€ë“¤ì´ ì˜ì„¸ê·¸ë©˜íŠ¸ë¨(`WS`)ê³¼ ì˜ëª»ì„¸ê·¸ë©˜íŠ¸ë¨(`PS`)ë¡œ ë¼ë²¨í•œ ì„¸í¬ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ì†Œê°œí–ˆì—ˆìŠµë‹ˆë‹¤. ì˜/ì˜ëª» ì„¸ê·¸ë©˜íŠ¸ëœ ì´ë¯¸ì§€ì¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ [ëœë¤í¬ë ˆìŠ¤íŠ¸ëª¨ë¸](/start/resampling/#modeling)ì„ í›ˆë ¨í•´ì„œ ìƒë¬¼í•™ìê°€ ì˜ëª» ì„¸ê·¸ë©˜íŠ¸ëœ ì„¸í¬ì´ë¯¸ì§€ë“¤ì„ ë¶„ì„ì—ì„œ í•„í„°ë§í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì´ ë°ì´í„°ì…‹ì— ìš°ë¦¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ [ë¦¬ìƒ˜í”Œë§](/start/resampling/#resampling)ì„ ì‚¬ìš©í–ˆì—ˆìŠµë‹ˆë‹¤.


```r
data(cells, package = "modeldata")
cells
#> # A tibble: 2,019 Ã— 58
#>   case  class angle_ch_1 area_ch_1 avg_inten_ch_1 avg_inten_ch_2 avg_inten_ch_3
#>   <fct> <fct>      <dbl>     <int>          <dbl>          <dbl>          <dbl>
#> 1 Test  PS        143.         185           15.7           4.95           9.55
#> 2 Train PS        134.         819           31.9         207.            69.9 
#> 3 Train WS        107.         431           28.0         116.            63.9 
#> 4 Train PS         69.2        298           19.5         102.            28.2 
#> 5 Test  PS          2.89       285           24.3         112.            20.5 
#> # â€¦ with 2,014 more rows, and 51 more variables: avg_inten_ch_4 <dbl>,
#> #   convex_hull_area_ratio_ch_1 <dbl>, convex_hull_perim_ratio_ch_1 <dbl>,
#> #   diff_inten_density_ch_1 <dbl>, diff_inten_density_ch_3 <dbl>, â€¦
```

## ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜ˆì¸¡í•˜ê¸°, ë” ì •í™•íˆ {#why-tune}

ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì€ íŠ¸ë¦¬-ê¸°ë°˜ ì•™ìƒë¸” ë°©ë²•ì´ê³  ë³´í†µ [ê¸°ë³¸ê°’ í•˜ì´í¼íŒŒë¼ë¯¸í„°](https://bradleyboehmke.github.io/HOML/random-forest.html#out-of-the-box-performance)ë¡œë„ ì„±ëŠ¥ì´ ë‚˜ì˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, [boosted tree models](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting) or [decision tree models](https://en.wikipedia.org/wiki/Decision_tree) ê°™ì€ ë‹¤ë¥¸ íŠ¸ë¦¬ê¸°ë°˜ ëª¨ë¸ë“¤ì€ ì •í™•ë„ê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ë“¤ì— ë¯¼ê°í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ ì¥ì—ì„œ **decision tree** ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•  ê²ƒì…ë‹ˆë‹¤. decision tree ì—ëŠ” íŠœë‹í•  ìˆ˜ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ëª‡ê°œê°€ ìˆìŠµë‹ˆë‹¤. í•œë²ˆ ì‚´í´ë´…ì‹œë‹¤:

- the complexity parameter (`cost_complexity` in tidymodels ì—ì„œ `cost_complexity` ë¼ê³  ë¶€ë¦„) for the tree, and
- the maximum `tree_depth`.

ì´ëŸ¬í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ë©´ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ”ë° decision tree ëª¨ë¸ì€ [overfitting](https://bookdown.org/max/FES/important-concepts.html#overfitting)ë˜ëŠ” ê²½í–¥ì´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ íŠ¸ë¦¬ëª¨ë¸ì€ íŠ¸ë ˆì´ë‹ ë°ì´í„°ì— _ë„ˆë¬´ ì˜_ ì í•©ë˜ëŠ” ê²½í–¥ì´ ìˆê¸° ë•Œë¬¸ì— ê·¸ë ‡ìŠµë‹ˆë‹¤. &mdash; ì‚¬ì‹¤ íŠ¸ë ˆì´ë‹ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” íŒ¨í„´ë“¤ì„ ê³¼í•™ìŠµí•´ì„œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•  ë•Œ ë°©í•´ê°€ ë  ì •ë„ê°€ ë©ë‹ˆë‹¤.

ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•  ê²ƒì…ë‹ˆë‹¤. `cost_complexity` ì˜ ê°’ì„ íŠœë‹í•˜ë©´ ìš°ë¦¬ íŠ¸ë¦¬ë¥¼  [pruning](https://bradleyboehmke.github.io/HOML/DT.html#pruning) í•˜ì—¬ ë„ì›€ì´ ë©ë‹ˆë‹¤. ë” ë³µì¡í•œ íŠ¸ë¦¬ì˜ ì—ëŸ¬ ë ˆì´íŠ¸ì— ì½”ìŠ¤íŠ¸ í˜¹ì€ í˜ë„í‹°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤; 0ì— ê°€ê¹Œìš´ ì½”ìŠ¤íŠ¸ëŠ” í”„ë£¬ëœ íŠ¸ë¦¬ë…¸ë“œ ê°œìˆ˜ë¥¼ ê°ì†Œì‹œí‚¤ê³  ê³¼ì í•©ëœ ë‚˜ë¬´ë¥¼ ì œê³µí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë†’ì€ ì½”ìŠ¤íŠ¸ëŠ” í”„ë£¬ëœ íŠ¸ë¦¬ ë…¸ë“œì˜ ê°œìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ê³  ìƒë°˜ëœ ë¬¸ì œ&mdash;an underfit tree ë¥¼ ì‚°ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´ì— `tree_depth` ë¥¼ íŠœë‹í•˜ë©´ ìš°ë¦¬ íŠ¸ë¦¬ë¥¼ ì–´ë–¤ ëìŠ¤ì— ë‹¤ë‹¤ë¥¸ ë’¤ ë” ìë¼ëŠ” ê²ƒì„ [ë°©ì§€](https://bradleyboehmke.github.io/HOML/DT.html#early-stopping) í•˜ëŠ” ë„ì›€ì„ ì¤ë‹ˆë‹¤. ìš°ë¦¬ì˜ ëª©ì ì€ ì´ëŸ¬í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ íŠœë‹í•˜ì—¬ ìš°ë¦¬ëª¨ë¸ì´ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ê°€ì¥ ì˜ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ê°’ë“¤ë¡œ íŠœë‹í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

íŠœë‹ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•˜ê¸° ì „ì—, í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë³¸ê°’ìœ¼ë¡œ ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼°ì„ ë•Œì™€ ê°™ì´ ìš°ë¦¬ ë°ì´í„°ë¥¼ íŠ¸ë ˆì´ë‹ì…‹ê³¼ í…ŒìŠ¤íŠ¸ ì…‹ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤. [ì „](/start/resampling/)ê³¼ ê°™ì´ `strata = class` ë¥¼ í•˜ì—¬ ì¸µí™” ìƒ˜í”Œë§ì„ ì´ìš©í•˜ì—¬ íŠ¸ë ˆì´ë‹ê³¼ í…ŒìŠ¤íŒ… ì…‹ì´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¢…ë¥˜ë¹„ìœ¨ì´ ê°™ë„ë¡ í•©ë‹ˆë‹¤.


```r
set.seed(123)
cell_split <- initial_split(cells %>% select(-case), 
                            strata = class)
cell_train <- training(cell_split)
cell_test  <- testing(cell_split)
```

ëª¨ë¸ì„ íŠœë‹í•˜ê¸° ìœ„í•´ íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

## í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ {#tuning}

[`decision_tree()`](https://parsnip.tidymodels.org/reference/decision_tree.html) ëª¨ë¸ì„ [rpart](https://cran.r-project.org/web/packages/rpart/index.html) ì—”ì§„ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ parsnip íŒ¨í‚¤ì§€ë¡œ ì‹œì‘í•´ ë´…ì‹œë‹¤. decision tree í•˜ì´í¼íŒŒë¼ë¯¸í„° `cost_complexity` and `tree_depth` ë¥¼ íŠœë‹í•˜ê¸° ìœ„í•´, íŠœë‹í•˜ê³  ì‹¶ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‹ë³„í•˜ëŠ” ëª¨ë¸ spec ì„ ìƒì„±í•©ë‹ˆë‹¤. 


```r
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tune_spec
#> Decision Tree Model Specification (classification)
#> 
#> Main Arguments:
#>   cost_complexity = tune()
#>   tree_depth = tune()
#> 
#> Computational engine: rpart
```

ì—¬ê¸°ì„œ `tune()` ë¥¼ placeholder ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤. íŠœë‹ í”„ë¡œì„¸ìŠ¤ í›„, ì´ëŸ¬í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°ê°ì— ìˆ˜ì¹˜ê°’ í•˜ë‚˜ì”©ì„ ê²°ì •í•  ê²ƒì…ë‹ˆë‹¤. í˜„ì¬ëŠ” ìš°ë¦¬ parsnip ëª¨ë¸ ê°ì²´ë¥¼ ëª…ì‹œí•˜ê³  ìš°ë¦¬ê°€ `tune()` í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.

(ì „ì²´ íŠ¸ë ˆì´ë‹ì…‹ê°™ì€) í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ì— ì´ ìŠ¤í™ì„ íŠ¸ë ˆì´ë‹í•˜ê³  ì–´ë–¤ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì´ ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ í•™ìŠµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹ , ìš°ë¦¬ëŠ” ë¦¬ìƒ˜í”Œëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì—¬ëŸ¬ê°œë¥¼ í›ˆë ¨í•˜ê³  ì–´ë–¤ ëª¨ë¸ì´ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆëŠ”ì§€ ë³¼ _ìˆ˜ ìˆìŠµë‹ˆë‹¤._ ë ˆê·¤ëŸ¬ ê·¸ë¦¬ë“œ ê°’ì„ ìƒì„±í•˜ì—¬ ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— í¸ë¦¬í•œ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```r
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
```

[`grid_regular()`](https://dials.tidymodels.org/reference/grid_regular.html) í•¨ìˆ˜ëŠ” [dials](https://dials.tidymodels.org/) íŒ¨í‚¤ì§€ì— ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ì‹œë„í•´ë³¼ í•©ë¦¬ì ì¸ ê°’ë“¤ì„ ì„ íƒí•©ë‹ˆë‹¤; ì—¬ê¸°ì„œëŠ” ë‘ ê²½ìš°ì— 5ë¥¼ ì‹œë„í•©ë‹ˆë‹¤. ë‘ ê°œë¥¼ íŠœë‹í•˜ë¯€ë¡œ, `grid_regular()` ëŠ” 5 `\(\times\)` 5 = 25 ê°œì˜ ê°ê¸° ë‹¤ë¥¸ íŠœë‹ ì¡°í•©ì„ íƒ€ì´ë”” í‹°ë¸” í¬ë§·ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.


```r
tree_grid
#> # A tibble: 25 Ã— 2
#>    cost_complexity tree_depth
#>              <dbl>      <int>
#>  1    0.0000000001          1
#>  2    0.0000000178          1
#>  3    0.00000316            1
#>  4    0.000562              1
#>  5    0.1                   1
#>  6    0.0000000001          4
#>  7    0.0000000178          4
#>  8    0.00000316            4
#>  9    0.000562              4
#> 10    0.1                   4
#> # â€¦ with 15 more rows
```

Here, you can see all 5 values of `cost_complexity` ranging up to 0.1. These values get repeated for each of the 5 values of `tree_depth`:


```r
tree_grid %>% 
  count(tree_depth)
#> # A tibble: 5 Ã— 2
#>   tree_depth     n
#>        <int> <int>
#> 1          1     5
#> 2          4     5
#> 3          8     5
#> 4         11     5
#> 5         15     5
```


Armed with our grid filled with 25 candidate decision tree models, let's create [cross-validation folds](/start/resampling/) for tuning:


```r
set.seed(234)
cell_folds <- vfold_cv(cell_train)
```

Tuning in tidymodels requires a resampled object created with the [rsample](https://rsample.tidymodels.org/) package.

## ê·¸ë¦¬ë“œ ëª¨ë¸íŠœë‹ {#tune-grid}

We are ready to tune! Let's use [`tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html) to fit models at all the different values we chose for each tuned hyperparameter. There are several options for building the object for tuning:

+ Tune a model specification along with a recipe or model, or 

+ Tune a [`workflow()`](https://workflows.tidymodels.org/) that bundles together a model specification and a recipe or model preprocessor. 

Here we use a `workflow()` with a straightforward formula; if this model required more involved data preprocessing, we could use `add_recipe()` instead of `add_formula()`.


```r
set.seed(345)

tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(class ~ .)

tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = cell_folds,
    grid = tree_grid
    )

tree_res
#> # Tuning results
#> # 10-fold cross-validation 
#> # A tibble: 10 Ã— 4
#>    splits             id     .metrics          .notes          
#>    <list>             <chr>  <list>            <list>          
#>  1 <split [1362/152]> Fold01 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
#>  2 <split [1362/152]> Fold02 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
#>  3 <split [1362/152]> Fold03 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
#>  4 <split [1362/152]> Fold04 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
#>  5 <split [1363/151]> Fold05 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
#>  6 <split [1363/151]> Fold06 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
#>  7 <split [1363/151]> Fold07 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
#>  8 <split [1363/151]> Fold08 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
#>  9 <split [1363/151]> Fold09 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
#> 10 <split [1363/151]> Fold10 <tibble [50 Ã— 6]> <tibble [0 Ã— 1]>
```

Once we have our tuning results, we can both explore them through visualization and then select the best result. The function `collect_metrics()` gives us a tidy tibble with all the results. We had 25 candidate models and two metrics, `accuracy` and `roc_auc`, and we get a row for each `.metric` and model. 


```r
tree_res %>% 
  collect_metrics()
#> # A tibble: 50 Ã— 8
#>    cost_complexity tree_depth .metric  .estimator  mean     n std_err .config   
#>              <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>     
#>  1    0.0000000001          1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦
#>  2    0.0000000001          1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦
#>  3    0.0000000178          1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦
#>  4    0.0000000178          1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦
#>  5    0.00000316            1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦
#>  6    0.00000316            1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦
#>  7    0.000562              1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦
#>  8    0.000562              1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦
#>  9    0.1                   1 accuracy binary     0.732    10  0.0148 Preprocesâ€¦
#> 10    0.1                   1 roc_auc  binary     0.777    10  0.0107 Preprocesâ€¦
#> # â€¦ with 40 more rows
```

We might get more out of plotting these results:


```r
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

<img src="figs/best-tree-1.svg" width="768" />

We can see that our "stubbiest" tree, with a depth of 1, is the worst model according to both metrics and across all candidate values of `cost_complexity`. Our deepest tree, with a depth of 15, did better. However, the best tree seems to be between these values with a tree depth of 4. The [`show_best()`](https://tune.tidymodels.org/reference/show_best.html) function shows us the top 5 candidate models by default:


```r
tree_res %>%
  show_best("accuracy")
#> # A tibble: 5 Ã— 8
#>   cost_complexity tree_depth .metric  .estimator  mean     n std_err .config    
#>             <dbl>      <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>      
#> 1    0.0000000001          4 accuracy binary     0.807    10  0.0119 Preprocessâ€¦
#> 2    0.0000000178          4 accuracy binary     0.807    10  0.0119 Preprocessâ€¦
#> 3    0.00000316            4 accuracy binary     0.807    10  0.0119 Preprocessâ€¦
#> 4    0.000562              4 accuracy binary     0.807    10  0.0119 Preprocessâ€¦
#> 5    0.1                   4 accuracy binary     0.786    10  0.0124 Preprocessâ€¦
```

We can also use the [`select_best()`](https://tune.tidymodels.org/reference/show_best.html) function to pull out the single set of hyperparameter values for our best decision tree model:


```r
best_tree <- tree_res %>%
  select_best("accuracy")

best_tree
#> # A tibble: 1 Ã— 3
#>   cost_complexity tree_depth .config              
#>             <dbl>      <int> <chr>                
#> 1    0.0000000001          4 Preprocessor1_Model06
```

These are the values for `tree_depth` and `cost_complexity` that maximize accuracy in this data set of cell images. 


## Finalizing our model {#final-model}

We can update (or "finalize") our workflow object `tree_wf` with the values from `select_best()`. 


```r
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)

final_wf
#> â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#> Preprocessor: Formula
#> Model: decision_tree()
#> 
#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#> class ~ .
#> 
#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#> Decision Tree Model Specification (classification)
#> 
#> Main Arguments:
#>   cost_complexity = 1e-10
#>   tree_depth = 4
#> 
#> Computational engine: rpart
```

Our tuning is done!

### The last fit

Finally, let's fit this final model to the training data and use our test data to estimate the model performance we expect to see with new data. We can use the function [`last_fit()`](https://tune.tidymodels.org/reference/last_fit.html) with our finalized model; this function _fits_ the finalized model on the full training data set and _evaluates_ the finalized model on the testing data.


```r
final_fit <- 
  final_wf %>%
  last_fit(cell_split) 

final_fit %>%
  collect_metrics()
#> # A tibble: 2 Ã— 4
#>   .metric  .estimator .estimate .config             
#>   <chr>    <chr>          <dbl> <chr>               
#> 1 accuracy binary         0.802 Preprocessor1_Model1
#> 2 roc_auc  binary         0.840 Preprocessor1_Model1

final_fit %>%
  collect_predictions() %>% 
  roc_curve(class, .pred_PS) %>% 
  autoplot()
```

<img src="figs/last-fit-1.svg" width="672" />

The performance metrics from the test set indicate that we did not overfit during our tuning procedure.

The `final_fit` object contains a finalized, fitted workflow that you can use for predicting on new data or further understanding the results. You may want to extract this object, using [one of the `extract_` helper functions](https://tune.tidymodels.org/reference/extract-tune.html).


```r
final_tree <- extract_workflow(final_fit)
final_tree
#> â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#> Preprocessor: Formula
#> Model: decision_tree()
#> 
#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#> class ~ .
#> 
#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#> n= 1514 
#> 
#> node), split, n, loss, yval, (yprob)
#>       * denotes terminal node
#> 
#>  1) root 1514 539 PS (0.64398943 0.35601057)  
#>    2) total_inten_ch_2< 41732.5 642  33 PS (0.94859813 0.05140187)  
#>      4) shape_p_2_a_ch_1>=1.251801 631  27 PS (0.95721078 0.04278922) *
#>      5) shape_p_2_a_ch_1< 1.251801 11   5 WS (0.45454545 0.54545455) *
#>    3) total_inten_ch_2>=41732.5 872 366 WS (0.41972477 0.58027523)  
#>      6) fiber_width_ch_1< 11.37318 406 160 PS (0.60591133 0.39408867)  
#>       12) avg_inten_ch_1< 145.4883 293  85 PS (0.70989761 0.29010239) *
#>       13) avg_inten_ch_1>=145.4883 113  38 WS (0.33628319 0.66371681)  
#>         26) total_inten_ch_3>=57919.5 33  10 PS (0.69696970 0.30303030) *
#>         27) total_inten_ch_3< 57919.5 80  15 WS (0.18750000 0.81250000) *
#>      7) fiber_width_ch_1>=11.37318 466 120 WS (0.25751073 0.74248927)  
#>       14) eq_ellipse_oblate_vol_ch_1>=1673.942 30   8 PS (0.73333333 0.26666667)  
#>         28) var_inten_ch_3>=41.10858 20   2 PS (0.90000000 0.10000000) *
#>         29) var_inten_ch_3< 41.10858 10   4 WS (0.40000000 0.60000000) *
#>       15) eq_ellipse_oblate_vol_ch_1< 1673.942 436  98 WS (0.22477064 0.77522936) *
```

We can create a visualization of the decision tree using another helper function to extract the underlying engine-specific fit.


```r
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

<img src="figs/rpart-plot-1.svg" width="768" />

Perhaps we would also like to understand what variables are important in this final model. We can use the [vip](https://koalaverse.github.io/vip/) package to estimate variable importance [based on the model's structure](https://koalaverse.github.io/vip/reference/vi_model.html#details). 


```r
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```

<img src="figs/vip-1.svg" width="576" />

These are the automated image analysis measurements that are the most important in driving segmentation quality predictions.


We leave it to the reader to explore whether you can tune a different decision tree hyperparameter. You can explore the [reference docs](/find/parsnip/#models), or use the `args()` function to see which parsnip object arguments are available:


```r
args(decision_tree)
#> function (mode = "unknown", engine = "rpart", cost_complexity = NULL, 
#>     tree_depth = NULL, min_n = NULL) 
#> NULL
```

You could tune the other hyperparameter we didn't use here, `min_n`, which sets the minimum `n` to split at any node. This is another early stopping method for decision trees that can help prevent overfitting. Use this [searchable table](/find/parsnip/#model-args) to find the original argument for `min_n` in the rpart package ([hint](https://stat.ethz.ch/R-manual/R-devel/library/rpart/html/rpart.control.html)). See whether you can tune a different combination of hyperparameters and/or values to improve a tree's ability to predict cell segmentation quality.



## Session information


```
#> â”€ Session info  ğŸ‘  ğŸ‘©â€ğŸš’  ğŸ¥   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  hash: ewe, woman firefighter, kiwi fruit
#> 
#>  setting  value
#>  version  R version 4.1.1 (2021-08-10)
#>  os       macOS Big Sur 10.16
#>  system   x86_64, darwin17.0
#>  ui       X11
#>  language (EN)
#>  collate  en_US.UTF-8
#>  ctype    en_US.UTF-8
#>  tz       Asia/Seoul
#>  date     2021-12-21
#>  pandoc   2.11.4 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)
#> 
#> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  package    * version date (UTC) lib source
#>  broom      * 0.7.10  2021-10-31 [1] CRAN (R 4.1.0)
#>  dials      * 0.0.10  2021-09-10 [1] CRAN (R 4.1.0)
#>  dplyr      * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)
#>  ggplot2    * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)
#>  infer      * 1.0.0   2021-08-13 [1] CRAN (R 4.1.0)
#>  parsnip    * 0.1.7   2021-07-21 [1] CRAN (R 4.1.0)
#>  purrr      * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)
#>  recipes    * 0.1.17  2021-09-27 [1] CRAN (R 4.1.0)
#>  rlang      * 0.4.12  2021-10-18 [1] CRAN (R 4.1.0)
#>  rpart      * 4.1-15  2019-04-12 [1] CRAN (R 4.1.1)
#>  rpart.plot * 3.1.0   2021-07-24 [1] CRAN (R 4.1.0)
#>  rsample    * 0.1.1   2021-11-08 [1] CRAN (R 4.1.0)
#>  tibble     * 3.1.6   2021-11-07 [1] CRAN (R 4.1.0)
#>  tidymodels * 0.1.4   2021-10-01 [1] CRAN (R 4.1.0)
#>  tune       * 0.1.6   2021-07-21 [1] CRAN (R 4.1.0)
#>  vip        * 0.3.2   2020-12-17 [1] CRAN (R 4.1.0)
#>  workflows  * 0.2.4   2021-10-12 [1] CRAN (R 4.1.0)
#>  yardstick  * 0.0.9   2021-11-22 [1] CRAN (R 4.1.0)
#> 
#>  [1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library
#> 
#> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```
