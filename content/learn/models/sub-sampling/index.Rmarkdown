---
title: "Class 불균형 상황에서 서브샘플링"
tags: [recipes, themis, discrim, parsnip]
categories: [model fitting, pre-processing]
type: learn-subsection
weight: 3
description: | 
  언더샘플링과 오버샘플링을 통해 불균형 데이터셋에서 모델 성능을 개선한다.
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
source(here::here("content/learn/common.R"))
```

```{r load, include = FALSE, message = FALSE, warning = FALSE}
library(readr)
library(klaR)
library(tidymodels)
library(discrim)
library(themis)
library(ROSE)

pkgs <- c("tidymodels", "klaR", "themis", "discrim", "readr", "ROSE")

theme_set(theme_bw() + theme(legend.position = "top"))
```


## 들어가기

`r req_pkgs(pkgs)`

적절한 클래스를 언더샘플링하거나 오버샘플링 하는, 훈련데이테섯 서브샘플링은 하나 이상의 클래스가 잘 나오지 않는 classification 데이터를 다루는데 도움이 될 수 있습니다. 이러한 상황에서 (보충하지 않으면) 대부분의 모델은 다수 클래스에 과적합될 수 있고, 다수 클래스에 대해서는 매우 좋은 통계량을 산출하지만, 소수 클래스들에 대해서는 낮은 성적을 보여줍니다.

이 문서는 클래스 임밸런스를 다루는 서브샘플링을 설명합니다. 더 잘 이해하기 위해, 민감도(sensitivity), 특이도(specificity), roc 커브와 같은 classification 지표들에 대해 지식이 조금 필요합니다. [Kuhn and Johnson (2019)](https://bookdown.org/max/FES/measuring-performance.html) 의 섹션 3.2.2 에서 이러한 지표들에 대해 자세히 알아보세요.

## 시뮬레이션 데이터

첫번째 클래스가 거의 일어나지 않는 두 클래스 문제를 고려해봅시다. 데이터는 시뮬레이션 되었고, 아래 코드를 이용해서 R 로 불러올 수 있습니다.

```{r load-data, message = FALSE}
imbal_data <- 
  readr::read_csv("https://bit.ly/imbal_data") %>% 
  mutate(Class = factor(Class))
dim(imbal_data)
table(imbal_data$Class)
```

"Class1" 가 관심있는 이벤트라면, 어떤 classification 모델은 매우 좋은 _특이도_ 를 갖게 되기 쉬울 것이데, 데이터 대부분이 두번째 클래스이기 때문이다. 

그러나, _민감도_ 가 낮을 가능성이 큰데, 모델이 모든 것을 다수 클래스로 예측해서 정확도(혹은 로스 함수)를 최적화할 것이기 때문이다.

클래스 불균형의 결과 중 하나는 기본값 확률 컷오프를 50%로 하는 것이 부적절하다는 것입니다. 더 극단적인 컷오프값이 성능이 더 좋을 수 있습니다. 

## 데이터 서브샘플링하기

이 이슈를 누그러뜨리는 방법 중 하나는 데이터를 _서브샘플링_ 하는 것입니다. 이를 수행하는 방법은 많지만, 가장 간단한 방법은 다수 클래스와 소수 클래스가 같은 빈도가 될 때 까지 _다운샘플링_ (undersample) 하는 것입니다. 직관과 반하는 것 같지만, 데이터 많은 부분을 버리는 것은 다수와 소수 클래스를 모두 인식하는 유용한 모델을 만드는 것에 효과적일 수 있습니다. 어떤 경우, 모델의 전체 성능이 더 나아지는 것을 의미합니다. (예 ROC 커브 아래 면적이 개선됨) 하지만, 서브샘플링은 더 잘 캘리브레이트되는 모델을 항상 산출하는데 이는 클래스 확률의 분포가 더 잘 작동한다는 것을 의미합니다. 결과로 기본값 50% 컷오프 값은 민감도와 특이도가 더 나아집니다. 

우리 시뮬레이션 데이터를 위한 레시피에 있는 `themis::step_rose()` 를 사용한 서브샘플링을 탐색해봅시다. [Menardi, G. and Torelli, N. (2014)](https://scholar.google.com/scholar?hl=en&q=%22training+and+assessing+classification+rules+with+imbalanced+data%22) 의 ROSE (random over sampling examples) 방법을 사용합니다. 언더샘플링이 아닌 오버샘플링의 예입니다.


워크플로우는:

 * 서브샘플링은 _리샘플링 내_에서 일어난다는 것이 매유 중요합니다. 그렇지 않으면, 리샘플링프로세스는 [모델성능이 안좋을](https://topepo.github.io/caret/subsampling-for-class-imbalances.html#resampling) 수 있습니다. 
 * 서브샘플링은 분석셋에 적용되어야만 합니다. 측정 셋은 이벤트 빈도가 "야생"에서 측정도니 이벤트 빈도를 반영해야하고, 이러한 이유로  argument to `step_downsample()` 와 다른 서브샘플링 단계의 `skip` 인수는 기본값 `TRUE` 를 같습니다. 

다음은 오버샘플링을 구현하는 간단한 레시피이다:

```{r rec}
library(tidymodels)
library(themis)
imbal_rec <- 
  recipe(Class ~ ., data = imbal_data) %>%
  step_rose(Class)
```

[quadratic discriminant analysis](https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis) (QDA) 모델을 우리의 모델로 선택해 봅니다. discrim 패키지에서 다음을 이용해서 QDA 모델을 정의할 수 있습니다:

```{r qda}
library(discrim)
qda_mod <- 
  discrim_regularized(frac_common_cov = 0, frac_identity = 0) %>% 
  set_engine("klaR")
```

[workflow](https://tidymodels.github.io/workflows/) 에서 객체들을 묶을 수 있습니다:

```{r wflw}
qda_rose_wflw <- 
  workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(imbal_rec)
qda_rose_wflw
```

## 모델 성능

모델을 리샘플하는데 층화 10-fold cross-validation 을 사용합니다:

```{r cv}
set.seed(5732)
cv_folds <- vfold_cv(imbal_data, strata = "Class", repeats = 5)
```

모델 성능을 측정하기 위해 두개의 지표를 사용합시다:

 * Area under [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) 는 _모든_ 컷오프값을 통튼 전체 성능을 측정값입니다. 1 에 가까운 값은 매우 좋은 가까운 값은 매우 좋은 결과를 의미하고, 0.5 근처의 값은 모델 성능이 매우 좋지 않음을 의미합니다.  
 * _J_ 인덱스 (a.k.a. [Youden's _J_](https://en.wikipedia.org/wiki/Youden%27s_J_statistic) statistic) 는 `sensitivity + specificity - 1` 입니다. 이 지표도 1 과 가까울 수록 좋습니다.

모델이 잘 캘리브레인션되지 않을때도, ROC 커브값은 낮은 성능을 보여주지 않을 것입니다. 하지만, 클래스 확률에 대해 pathological 분포로 모델에 대해 _J_ 인덱스가 낮을 것입니다. (다시 번역필요) 이러한 지표들을 계산하기 위해 yardstick 패키지를 사용할 수 있습니다.

```{r metrics}
cls_metrics <- metric_set(roc_auc, j_index)
```

이제, 모델을 훈련하고 `tune::fit_resamples()` 로 결과를 생성합니다:

```{r resample-rose, message=FALSE}
set.seed(2180)
qda_rose_res <- fit_resamples(
  qda_rose_wflw, 
  resamples = cv_folds, 
  metrics = cls_metrics
)

collect_metrics(qda_rose_res)
```

ROSE 를 이용하지 않고 결과가 어떻게 생겼을까? 다른 워크플로를 생성하고 같은 리샘플과 함께 QDA 모델을 적합할 수 있습니다:

```{r qda-only}
qda_wflw <- 
  workflow() %>% 
  add_model(qda_mod) %>% 
  add_formula(Class ~ .)

set.seed(2180)
qda_only_res <- fit_resamples(qda_wflw, resamples = cv_folds, metrics = cls_metrics)
collect_metrics(qda_only_res)
```

ROSE 가 많이 도움을 주었는데 특히 J-인덱스에서 그렇습니다. 
클래스 불균형 샘플링 방법은 hard 클래스 예측 (즉, 범주형 예측)에 기반한 지표들을 크게 개선시키는 경향이 있습니다. 
왜냐하면 기본값 컷오프가 민감도와 특이도를 잘 균형을 맞추는 경향이 있기 때문입니다.

각 리샘플에 대한 지표들을 플롯하여 개별 결과가 어떻게 변했는지 보자.

```{r merge-metrics}
no_sampling <- 
  qda_only_res %>% 
  collect_metrics(summarize = FALSE) %>% 
  dplyr::select(-.estimator) %>% 
  mutate(sampling = "no_sampling")

with_sampling <- 
  qda_rose_res %>% 
  collect_metrics(summarize = FALSE) %>% 
  dplyr::select(-.estimator) %>% 
  mutate(sampling = "rose")

bind_rows(no_sampling, with_sampling) %>% 
  mutate(label = paste(id2, id)) %>%  
  ggplot(aes(x = sampling, y = .estimate, group = label)) + 
  geom_line(alpha = .4) + 
  facet_wrap(~ .metric, scales = "free_y")
```

서브샘플링이 대부분 hard 클래스 예측을 사용하는 지표들에 영향을 준다는 것을 시각적으로 보여주고 있습니다.

## 세션정보

```{r si, echo = FALSE}
small_session(pkgs)
```

