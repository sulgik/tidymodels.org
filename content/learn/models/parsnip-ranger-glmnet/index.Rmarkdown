---
title: "회귀 모델링의 두가지 방법"
tags: [rsample, parsnip]
categories: [model fitting]
type: learn-subsection
weight: 1
description: | 
  다른 연산 엔전을 가진 다른 종류의 회귀 모형을 생성하고 훈련시키기.
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
source(here::here("content/learn/common.R"))
```

```{r load, include=FALSE}
library(tidymodels)
library(ranger)
library(randomForest)
library(glmnet)

preds <- c("Longitude", "Latitude", "Lot_Area", "Neighborhood", "Year_Sold")
pred_names <- paste0("`", preds, "`")

pkgs <- c("tidymodels", "ranger", "randomForest", "glmnet")

theme_set(theme_bw() + theme(legend.position = "top"))
```


## 들어가기

`r req_pkgs(pkgs)`

우리는 tidymodels 패키지 [parsnip](https://tidymodels.github.io/parsnip/) 로 회귀 모델을 생성하고 연속형이나 수치 값을 예측할 수 있습니다. 
여기에서, 모든 인풋이 수치형일 _필요가 없는_ 랜덤포레스트 모델 ([여기 ](https://bookdown.org/max/FES/categorical-trees.html)에서 이에 관한 논의를 살펴보라)을 우선 적합하고, _data descripters_ 뿐만 아니라, `fit()` 과 `fit_xy()` 를 사용하는 법을 논의해 봅시다.

두번째로는, regularized 선형 회귀 모형을 적합하여 parsnip 을 이용하여 다른 유형의 모델 사이를 움직여봅시다.

## Ames 주택 데이터

Ames 주택 데이터를 사용하여 parsnip 을 이용하여 회귀모형을 생성해 볼 것입니다. 첫째로, 데이터셋을 준비하고 간단한 트레이닝/테스트셋 분리를 합니다:

```{r ames-split}
library(tidymodels)

data(ames)

set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price", prop = 0.75)

ames_train <- training(data_split)
ames_test  <- testing(data_split)
```

여기서 테스트셋을 사용하는 것은 _설명목적_ 입니다; 일반적으로 데이터 분석에서 이러한 테스트데이터는 저장된 후 다양한 모델을 평가한 후 맨 마지막에 사용됩니다.

## 랜덤 포레스트

랜덤포레스트를 파라미터 셋으로 적합하는 것부터 시작할 것입니다. 
`r knitr::combine_words(pred_names)` 개의 설명변수가 있는 모델을 생성합시다. 간단한 랜덤 포레스트 모델은 다음과 같이 설정할 수 있습니다:

```{r rf-basic}
rf_defaults <- rand_forest(mode = "regression")
rf_defaults
```

이 모델은 레인저 패키지 기본값으로 적합될 것입니다. `fit` 에 추가 인수를 넣지 않았기 때문에, _많은_ 인수들이 `ranger::ranger()` 함수로 부터 기본값으로 설정될 것입니다. 모델 함수의 도움말 페이지에서는 기본값 파라미터들을 기술하고 `translate()` 함수를 사용하여 이에 관한 세부사항을 확인할 수 있습니다. 

parsnip 패키지에는 모델을 적합하는 두가지 다른 인터페이스가 있습니다: 

- 공식(formula) 인터페이스 (`fit()`)
- 비공식 (non-formula) 인터페이스 (`fit_xy()`).

비공식 인터페이스부터 시작해봅니다:


```{r rf-basic-xy}
preds <- c("Longitude", "Latitude", "Lot_Area", "Neighborhood", "Year_Sold")

rf_xy_fit <- 
  rf_defaults %>%
  set_engine("ranger") %>%
  fit_xy(
    x = ames_train[, preds],
    y = log10(ames_train$Sale_Price)
  )

rf_xy_fit
```

비공식 인터페이스는 설명변수를 모델 함수에 전달하기 전에 설명변수에는 아무 것도 하지 않습니다. 이 특별한 모델은 indicator 변수 (때로 "더미변수" 로 불림) 를 모델 적합 전에 생성할 필요가 _없습니다._ 출력에서 "Number of independent variables: 5" 를 나타냈습니다.

회귀 모델에서 우리는 기본 `predict()` 방법을 사용할 수 있는데, 이는 `.pred` 라고 명명된 하나의 열이 있는 티블을 반환합니다.

```{r rf-basic-xy-pred}
test_results <- 
  ames_test %>%
  select(Sale_Price) %>%
  mutate(Sale_Price = log10(Sale_Price)) %>%
  bind_cols(
    predict(rf_xy_fit, new_data = ames_test[, preds])
  )
test_results %>% slice(1:5)

# summarize performance
test_results %>% metrics(truth = Sale_Price, estimate = .pred) 
```

주의할 사항은: 

 * 모델이 indicator 변수들을 필요로 했다면, 이들을 `fit()` 을 사용하기 전에 수동으로 생성해야할 것입니다. (recipes 패키지를 사용하던지 해서)
 * 모델링 전에 출력을 수동으로 로그했어야 합니다.

이제 새로운 파라미터 값들을 사용하여 공식 방법을 사용하는 것을 배워봅시다:

```{r rf-basic-form}
rand_forest(mode = "regression", mtry = 3, trees = 1000) %>%
  set_engine("ranger") %>%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```
 

ranger 대신 randomForest 패키지를 사용하고 싶다고 가정해 봅시다. 
공식에서 바뀌어야 하는 유일한 부분은 `set_engine()` 인수입니다:


```{r rf-rf}
rand_forest(mode = "regression", mtry = 3, trees = 1000) %>%
  set_engine("randomForest") %>%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```

프린트된 공식 코드 를 살펴봅시다; 첫번째 함수는 인수 이름 `ntree` 를 사용하고, 다른 함수는 `num.trees` 를 사용합니다. parsnip 모델들은 주 인수의 구체적인 이름들을 몰라도 됩니다. 

`mtry` 값을 데이터의 설명변수의 개수에 기반하여 수정하고 싶다고 가정합니다. 일반적으로, 좋은 기본값은 `floor(sqrt(num_predictors))` 이지만, 순수한 배깅 모델은 `mtry` 값이 파라미터 전체 숫자와 같기를 요구합니다. 모델이 적합될 때 얼마나 많은 설명변수가 있을 것인지 알 수 없는 경우가 있어서, 코드를 작성하기 전에 정확히 아는 것은 여러울 수 있습니다.


parsnip 이 모델을 적합할 때, [_data descriptors_](https://tidymodels.github.io/parsnip/reference/descriptors.html) 를 사용할 수 있게 됩니다. 이것들은 모델이 적합될 때 어떤 것을 사용할 수 잇는지 알려주려고 합니다. 모델 객체가 생성될 때 (예를 들어 `rand_forest()` 를 사용해서) 제공하는 인수 값들을 delay 하지 않는다면 _즉시 평가됩니다_. 인수평가를 지연시키기 위해서는, `rlang:expr()` 를 사용하여 표현형(expression)을 만들 수 있습니다.

우리 예제 모델이서 관련된 두 개의 데이터 descriptor 는:

 * `.preds()`: **더미변수 생성 이전의** 설명변수와 관련있는 데이터셋 내의 설명 _변수_ 의 개수.
 * `.cols()`: 더미 변수들 (혹은 기타 인코딩)이 생성된 후 설명변수 _열_의 개수.

ranger 는 indicator 값을 생성하지 않기 때문에, `.preds()` 는 배깅모델의 `mtry` 에 적절할 것입니다.

`.preds()` descriptor 가 있는 표현형을 사용하여 배깅 모델을 적합해 봅시다.

```{r bagged}
rand_forest(mode = "regression", mtry = .preds(), trees = 1000) %>%
  set_engine("ranger") %>%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```


## Regularized 회귀

선형 모델도 이 데이터셋에 잘 맞아 들어갈 것입니다. 
`linear_reg()` parsnip 모델을 사용할 수 있습니다.
regularization/penalization 을 수행할 수 있는 두 개의 엔진, glmnet 과 sparklyr 패키지가 있습니다. 
전자를 사용해 봅시다. 
glmnet 패키지는 비공식(non-formula) 방법만 구현하지만 parsnip 은 공식, 비공식 방법 모두 사용할 수 있게 합니다. 

regularization 이 사용될 때, 설명변수는 모델에 전달되기 전, 우선 센터링되고 스케일링 되어야 합니다. 공식 방법은 자동으로 이를 수행해주지 않으므로, 직접 해야합니다. 이러한 단계를 위해 [recipes](https://tidymodels.github.io/recipes/) 패키지를 사용할 것입니다.

```{r glmn-form}
norm_recipe <- 
  recipe(
    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, 
    data = ames_train
  ) %>%
  step_other(Neighborhood) %>% 
  step_dummy(all_nominal()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_log(Sale_Price, base = 10) %>% 
  # estimate the means and standard deviations
  prep(training = ames_train, retain = TRUE)

# Now let's fit the model using the processed version of the data

glmn_fit <- 
  linear_reg(penalty = 0.001, mixture = 0.5) %>% 
  set_engine("glmnet") %>%
  fit(Sale_Price ~ ., data = bake(norm_recipe, new_data = NULL))
glmn_fit
```

`penalty` 가 설정되지 않으면 모든 `lambda` 값이 계산될 것입니다. 
특정 `lambda` (aka `penalty`) 값에 대한 예측값을 얻으려면:

```{r glmn-pred}
# First, get the processed version of the test set predictors:
test_normalized <- bake(norm_recipe, new_data = ames_test, all_predictors())

test_results <- 
  test_results %>%
  rename(`random forest` = .pred) %>%
  bind_cols(
    predict(glmn_fit, new_data = test_normalized) %>%
      rename(glmnet = .pred)
  )
test_results

test_results %>% metrics(truth = Sale_Price, estimate = glmnet) 

test_results %>% 
  gather(model, prediction, -Sale_Price) %>% 
  ggplot(aes(x = prediction, y = Sale_Price)) + 
  geom_abline(col = "green", lty = 2) + 
  geom_point(alpha = .4) + 
  facet_wrap(~model) + 
  coord_fixed()
```

마지막 플롯에서 랜덤포레스트와 regularized 회귀모델의 성능을 비교합니다.

## 세션정보

```{r si, echo = FALSE}
small_session(pkgs)
```
 
