---
title: "Partial least squares 로 하는 다변량 분석"
tags: [recipes,rsample]
categories: [pre-processing]
type: learn-subsection
weight: 6
description: | 
  하나 이상의 결과가 있는 예측 모델을 만들고 적합하기.
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
source(here::here("content/learn/common.R"))
```

```{r load, include = FALSE}
library(pls)
library(tidymodels)
library(sessioninfo)
pkgs <- c("modeldata", "pls", "tidymodels")

theme_set(theme_bw() + theme(legend.position = "top"))
```


## 들어가기

`r req_pkgs(pkgs)`

"다변량 분석" 은 다중 _아웃컴_ 을 모델링하고, 분석하고 예측하는 것을 의미합니다. 
일반적인 통계 도구들에는 다변량 버전이 있습니다. 
예를 들어, 예측할 두 개의 아웃컴을 의미하는 `y1`, `y2` 열을 가진 데이터셋이 있다고 가정해봅시다. 
`lm()` 함수는 다음과 같이 생겼습니다:

```{r lm, eval = FALSE}
lm(cbind(y1, y2) ~ ., data = dat)
```

이 `cbind` 호출은 꽤 이상한데, 전통적 공식 인프라가 동작방법의 결과입니다.
recipes 패키지는 다루기 훨씬 쉽습니다!
이 장에서 다중 아웃컴을 모델링하는 법을 살펴볼 것입니다.

The data that we'll use has three outcomes. From `?modeldata::meats`:

> "These data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents.

> "For each meat sample the data consists of a 100 channel spectrum of absorbances and the contents of moisture (water), fat and protein. The absorbance is `-log10` of the transmittance measured by the spectrometer. The three contents, measured in percent, are determined by analytic chemistry."

The goal is to predict the proportion of the three substances using the chemistry test. There can often be a high degree of between-variable correlations in predictors, and that is certainly the case here. 

To start, let's take the two data matrices (called `endpoints` and `absorp`) and bind them together in a data frame:

```{r data}
library(modeldata)
data(meats)
```

The three _outcomes_ have fairly high correlations also. 

## Preprocessing the data

If the outcomes can be predicted using a linear model, partial least squares (PLS) is an ideal method. PLS models the data as a function of a set of unobserved _latent_ variables that are derived in a manner similar to principal component analysis (PCA). 

PLS, unlike PCA, also incorporates the outcome data when creating the PLS components. Like PCA, it tries to maximize the variance of the predictors that are explained by the components but it also tries to simultaneously maximize the correlation between those components and the outcomes. In this way, PLS _chases_ variation of the predictors and outcomes. 

Since we are working with variances and covariances, we need to standardize the data. The recipe will center and scale all of the variables. 

Many base R functions that deal with multivariate outcomes using a formula require the use of `cbind()` on the left-hand side of the formula to work with the traditional formula methods. In tidymodels, recipes do not; the outcomes can be symbolically "added" together on the left-hand side:

```{r recipe}
norm_rec <- 
  recipe(water + fat + protein ~ ., data = meats) %>%
  step_normalize(everything()) 
```

Before we can finalize the PLS model, the number of PLS components to retain must be determined. This can be done using performance metrics such as the root mean squared error. However, we can also calculate the proportion of variance explained by the components for the _predictors and each of the outcomes_. This allows an informed choice to be made based on the level of evidence that the situation requires. 

Since the data set isn't large, let's use resampling to measure these proportions. With ten repeats of 10-fold cross-validation, we build the PLS model on 90% of the data and evaluate on the heldout 10%. For each of the 100 models, we extract and save the proportions. 

The folds can be created using the [rsample](https://tidymodels.github.io/rsample/) package and the recipe can be estimated for each resample using the [`prepper()`](https://tidymodels.github.io/rsample/reference/prepper.html) function: 

```{r cv}
set.seed(57343)
folds <- vfold_cv(meats, repeats = 10)

folds <- 
  folds %>%
  mutate(recipes = map(splits, prepper, recipe = norm_rec))
```

## Partial least squares

복잡한 부분은 다음과 같습니다:

1. 설명변수와 반응변수 포맷을 pls 패키지가 필요로 하는 포맷으로 바꾸고,
2. 비율을 추정하기. 

첫번째 부분에서, 표준화된 반응변수와 설명변수가 두 개의 행렬로 포맷될 필요가 있습니다. 
레시피를 준비할 때, `retain = TRUE` 를 사용했기 때문에, 처리된 데이터를 다시 얻기 위해, `new_data = NULL` 와 함께, `bake()` 를 사용할 수 있습니다.
데이터를 행렬로 저장하기 위해, `composition = "matrix"` 옵션을 하면, 데이터를 티블로 저장하지 않고 필요한 포맷을 사용하게 됩니다.

pls 패키지는 간단한 공식이 모델을 규정하기를 기대하지만, 공식의 각 사이드는 _행렬을 표현_ 해야 합니다. 
다른말로 하면, 각 열이 행렬인 열이 두 개인 데이터셋이 필요합니다.
이를 하는 비밀은 두 행렬을 데이터프레임으로 추가할 때 `I()` 를 사용하여 두 행렬을 "보호하는 것"입니다.

설명되는 분산의 비율 계산은 설명변수에 대해서 간단합니다; `pls::explvar()` 함수가 계산할 수 있습니다. 
반응변수에 대해 이 과정은 더 복잡합니다.
이를 계산하는 준비된 함수는 명확하지는 않지만 계산하는 요약함수 내에 코드가 조금 있습니다 (아래 참고).


여기서 보이는 `get_var_explained()` 함수는 이 모든 계산을 하고 (설명변수, water, 등의) `components`, `source` 열과 이 성분들이 설명하는 변수의 비율 (`proportion`) 열이 있는 데이터프레임을 반환할 것입니다.


```{r var-explained}
library(pls)

get_var_explained <- function(recipe, ...) {
  
  # Extract the predictors and outcomes into their own matrices
  y_mat <- bake(recipe, new_data = NULL, composition = "matrix", all_outcomes())
  x_mat <- bake(recipe, new_data = NULL, composition = "matrix", all_predictors())
  
  # The pls package prefers the data in a data frame where the outcome
  # and predictors are in _matrices_. To make sure this is formatted
  # properly, use the `I()` function to inhibit `data.frame()` from making
  # all the individual columns. `pls_format` should have two columns.
  pls_format <- data.frame(
    endpoints = I(y_mat),
    measurements = I(x_mat)
  )
  # Fit the model
  mod <- plsr(endpoints ~ measurements, data = pls_format)
  
  # Get the proportion of the predictor variance that is explained
  # by the model for different number of components. 
  xve <- explvar(mod)/100 

  # To do the same for the outcome, it is more complex. This code 
  # was extracted from pls:::summary.mvr. 
  explained <- 
    drop(pls::R2(mod, estimate = "train", intercept = FALSE)$val) %>% 
    # transpose so that components are in rows
    t() %>% 
    as_tibble() %>%
    # Add the predictor proportions
    mutate(predictors = cumsum(xve) %>% as.vector(),
           components = seq_along(xve)) %>%
    # Put into a tidy format that is tall
    pivot_longer(
      cols = c(-components),
      names_to = "source",
      values_to = "proportion"
    )
}
```

각 리샘플에 해당하는 데이터프레임을 계산하고 다른 열에 결과를 저장합니다.

```{r get-estimates}
folds <- 
  folds %>%
  mutate(var = map(recipes, get_var_explained),
         var = unname(var))
```

이 데이터를 추출하고 집계하기 위해, 간단한 행 바인딩을 사용하여 데이터를 수직으로 쌓을 수 있습니다.
대부분의 동작은 첫 15 개 성분에 일어나기 때문에, 데이터를 필터하고 _평균_ 비율을 계산해 봅시다.

```{r collapse-and-average}
variance_data <- 
  bind_rows(folds[["var"]]) %>%
  filter(components <= 15) %>%
  group_by(components, source) %>%
  summarize(proportion = mean(proportion))
```

아래 플롯에서는, 단백질 측정값이 중요하면, 반응변수의 표현을 좋게 하기 위해 10 개 정도의 성분이 필요할 것이라는 것을 의미합니다.
설명변수 분산은 하나의 성분을 사용하여 극단적으로 잘 포착되는 것을 주목하세요.
이 데이터의 상관관계가 높기 때문입니다.

```{r plot, fig.width=6, fig.height=4.25,  out.width = '100%'}
ggplot(variance_data, aes(x = components, y = proportion, col = source)) + 
  geom_line(alpha = 0.5, size = 1.2) + 
  geom_point() 
```


## Session information

```{r si, echo = FALSE}
small_session(pkgs)
```
 
