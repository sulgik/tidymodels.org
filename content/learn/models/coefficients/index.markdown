---
title: "ëª¨ë¸ ê³„ìˆ˜ ì‘ì—…í•˜ê¸°"
tags: [parsnip,tune,broom,workflows]
categories: [model fitting]
type: learn-subsection
weight: 5
description: | 
  ê³„ìˆ˜ê°€ ìˆëŠ” ëª¨ë¸ì„ ìƒì„±í•˜ê³ , ì í•©ëœ ëª¨ë¸ì—ì„œ ê³„ìˆ˜ë¥¼ ì¶”ì¶œí•˜ê³ , ì‹œê°í™”í•œë‹¤.
---



## ë“¤ì–´ê°€ê¸° 

í†µê³„ ëª¨ë¸ì€ ë‹¤ì–‘í•œ êµ¬ì¡°ë¥¼ ê°–ìŠµë‹ˆë‹¤.
ì–´ë–¤ ëª¨ë¸ì€ ê° í•­ë§ˆë‹¤ ê³„ìˆ˜(coefficient, weight)ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
ì´ëŸ¬í•œ ëª¨ë¸ì˜ ì‰¬ìš´ ì˜ˆëŠ” ì„ í˜• í˜¹ì€ ë¡œì§€ìŠ¤í‹±íšŒê·€ì´ì§€ë§Œ, ë” ë³µì¡í•œ ëª¨ë¸ (ì˜ˆ: ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬, MARS)ì—ë„ ëª¨ë¸ ê³„ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤.
ì›¨ì´íŠ¸ë‚˜ ê³„ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë¸ìœ¼ë¡œ ì‘ì—…í•  ë•Œ ì¶”ì •í•œ ê³„ìˆ˜ë¥¼ í™•ì¸í•˜ê³  ì‹¶ì€ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

ì´ ì¥ì—ì„œ tidymodels ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì í•© ê°ì²´ë¡œ ë¶€í„° ê³„ìˆ˜ ì¶”ì •ê°’ì„ ì¶”ì¶œí•˜ëŠ” ë²•ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.
ì´ ì¥ì˜ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´, ë‹¤ìŒì˜ íŒ¨í‚¤ì§€ë“¤ì„ ì¸ìŠ¤í†¨í•´ì•¼í•©ë‹ˆë‹¤: glmnet and tidymodels.

## ì„ í˜• íšŒê·€

ì„ í˜• íšŒê·€ëª¨ë¸ë¶€í„° ì‹œì‘í•´ ë´…ì‹œë‹¤:

`$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x_1 + \ldots + \hat{\beta}_px_p$$` 

`\(\beta\)`ëŠ” ê³„ìˆ˜ì´ê³  `\(x_j\)` ì€ ëª¨ë¸ ì„¤ëª…ë³€ìˆ˜ ì´ê±°ë‚˜ í”¼ì³ì…ë‹ˆë‹¤.

[ì‹œì¹´ê³  ê¸°ì°¨ ë°ì´í„°](https://bookdown.org/max/FES/chicago-intro.html) ì—ì„œ Clark ì™€ Lake ì—­ì˜ ìŠ¹ì°¨ë¥¼ ì„¸ ì—­ì˜ 14ì¼ ì´ì „ ìŠ¹ì°¨ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡í•´ ë´…ì‹œë‹¤.

modeldata íŒ¨í‚¤ì§€ì— ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤:


```r
library(tidymodels)
tidymodels_prefer()
theme_set(theme_bw())

data(Chicago)

Chicago <- Chicago %>% select(ridership, Clark_Lake, Austin, Harlem)
```

### ë‹¨ì¼ ëª¨ë¸

ë‹¨ì¼í•œ parsnip ëª¨ë¸ ê°ì²´ë¥¼ ì í•©í•˜ëŠ” ê²ƒë¶€í„° ì‹œì‘í•´ ë´…ì‹œë‹¤.
`linear_reg()` ë¥¼ í•˜ì—¬ ëª¨ë¸ specification ì„ ìƒì„±í•  ê²ƒì…ë‹ˆë‹¤. 

{{% note %}} The default engine is `"lm"` so no call to `set_engine()` is required. {{%/ note %}}

ê³µì‹ê³¼ ë°ì´í„°ì…‹ì´ ì£¼ì–´ì§ˆ ë•Œ, `fit()` í•¨ìˆ˜ëŠ” ëª¨ë¸ ê³„ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.


```r
lm_spec <- linear_reg()
lm_fit <- fit(lm_spec, ridership ~ ., data = Chicago)
lm_fit
#> parsnip model object
#> 
#> Fit time:  5ms 
#> 
#> Call:
#> stats::lm(formula = ridership ~ ., data = data)
#> 
#> Coefficients:
#> (Intercept)   Clark_Lake       Austin       Harlem  
#>       1.678        0.904        0.612       -0.555
```

`tidy()` ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì í•©ëœ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê°€ì¥ ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.
broom íŒ¨í‚¤ì§€ì— ìˆëŠ” ì´ í•¨ìˆ˜ëŠ” ê³„ìˆ˜ì™€, ì—°ê´€ëœ í†µê³„ëŸ‰ì„ ë°ì´í„°í”„ë ˆì„ì— í‘œì¤€í™”ëœ ì—´ì´ë¦„ê³¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤:


```r
tidy(lm_fit)
#> # A tibble: 4 Ã— 5
#>   term        estimate std.error statistic   p.value
#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>
#> 1 (Intercept)    1.68     0.156      10.7  1.11e- 26
#> 2 Clark_Lake     0.904    0.0280     32.3  5.14e-210
#> 3 Austin         0.612    0.320       1.91 5.59e-  2
#> 4 Harlem        -0.555    0.165      -3.36 7.85e-  4
```

ì´í›„ ì„¹ì…˜ì—ì„œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ë¦¬ìƒ˜í”Œë˜ê±°ë‚˜ íŠœë‹ëœ ëª¨ë¸

tidymodels í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” ë¦¬ìƒ˜í”Œë§ ë°©ë²•ë“¤ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ê²ƒì„ ê°•ì¡°í•©ë‹ˆë‹¤. 
ì‹œê²Œì—´ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ì´ ì´ ë°ì´í„°ì— ì ì ˆí•˜ì§€ë§Œ, ë°ì´í„°ë¥¼ ë¦¬ìƒ˜í”Œí•˜ëŠ” [bootstrap](https://www.tmwr.org/resampling.html#bootstrap) ë°©ë²•ì„ ì´ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
bootstrap ë°©ë²•ì€ í†µê³„ì  ì¶”ì •ê°’ì˜ ë¶ˆí™•ì‹¤ì„±ì„ í‰ê°€í•  ë•Œ í‘œì¤€ì ì¸ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ì…ë‹ˆë‹¤.

í”Œë¡¯ê³¼ ì•„ì›ƒí’‹ì„ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ ë‹¤ì„¯ bootstrap ë¦¬ìƒ˜í”Œì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. (ì›ë˜ëŠ” ë¯¿ì„ë§Œí•œ ì¶”ì •ê°’ì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ ê°œìˆ˜ì˜ ë¦¬ìƒ˜í”Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤).


```r
set.seed(123)
bt <- bootstraps(Chicago, times = 5)
```

ë¦¬ìƒ˜í”Œë§ì´ ë§Œë“  ë°ì´í„°ì…‹ì˜ ë‹¤ë¥¸ ì‹œë®¬ë ˆì´ì…˜ ë²„ì „ì— ë™ì¼í•œ ëª¨ë¸ì„ ì í•©ì‹œí‚µë‹ˆë‹¤. 
ì¶”ì²œí•˜ëŠ” ë°©ë²•ì€ tidymodels í•¨ìˆ˜ [`fit_resamples()`](https://www.tmwr.org/resampling.html#resampling-performance)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

{{% warning %}} The `fit_resamples()` function does not automatically save the model objects for each resample since these can be quite large and its main purpose is estimating performance. However, we can pass a function to `fit_resamples()` that _can_ save the model object or any other aspect of the fit. {{%/ warning %}}

ì´ í•¨ìˆ˜ëŠ” ì í•©ëœ [ì›Œí¬í”Œë¡œìš° ê°ì²´](https://www.tmwr.org/workflows.html) ë¥¼ í‘œí˜„í•˜ëŠ” ì¸ìˆ˜ë¥¼ ì…ë ¥ìœ¼ë¡œ í•©ë‹ˆë‹¤. (`fit_resamples()` ì— ì›Œí¬í”Œë¡œìš°ë¥¼ ì•Œë ¤ì£¼ì§€ ì•Šì„ì§€ë¼ë„ ê·¸ë ‡ìŠµë‹ˆë‹¤.)

ì´ì œ ëª¨ë¸ ì í•©ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ëª¨ë¸ ê°ì²´ì˜ ë‘ "ë ˆë²¨"ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

* parsnip ëª¨ë¸ê°ì²´: ë‚´ë¶€ ëª¨ë¸ê°ì²´ë¥¼ ë˜í•‘í•¨. `extract_fit_parsnip()` í•¨ìˆ˜ë¡œ ì¶”ì¶œí•¨. 

* `extract_fit_engine()` ë¥¼ í†µí•œ ë‚´ë¶€ ëª¨ë¸ê°ì²´ (aka ì—”ì§„ì í•©). 

í›„ì ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì´ ëª¨ë¸ê°ì²´ë¥¼ ì´ì „ì„¹ì…˜ì—ì„œ í–ˆë“¯ì´ íƒ€ì´ë””í•˜ê²Œ í•  ê²ƒì…ë‹ˆë‹¤. 
ì´ë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì»¨íŠ¸ë¡¤ í•¨ìˆ˜ì— ì¶”ê°€í•©ì‹œë‹¤.


```r
get_lm_coefs <- function(x) {
  x %>% 
    # get the lm model object
    extract_fit_engine() %>% 
    # transform its format
    tidy()
}
tidy_ctrl <- control_grid(extract = get_lm_coefs)
```

ì´í›„ ì´ ì¸ìˆ˜ë¥¼ `fit_resamples()` ì— ì „ë‹¬í•©ë‹ˆë‹¤:


```r
lm_res <- 
  lm_spec %>% 
  fit_resamples(ridership ~ ., resamples = bt, control = tidy_ctrl)
lm_res
#> # Resampling results
#> # Bootstrap sampling 
#> # A tibble: 5 Ã— 5
#>   splits              id         .metrics         .notes           .extracts    
#>   <list>              <chr>      <list>           <list>           <list>       
#> 1 <split [5698/2076]> Bootstrap1 <tibble [2 Ã— 4]> <tibble [0 Ã— 1]> <tibble [1 Ã—â€¦
#> 2 <split [5698/2098]> Bootstrap2 <tibble [2 Ã— 4]> <tibble [0 Ã— 1]> <tibble [1 Ã—â€¦
#> 3 <split [5698/2064]> Bootstrap3 <tibble [2 Ã— 4]> <tibble [0 Ã— 1]> <tibble [1 Ã—â€¦
#> 4 <split [5698/2082]> Bootstrap4 <tibble [2 Ã— 4]> <tibble [0 Ã— 1]> <tibble [1 Ã—â€¦
#> 5 <split [5698/2088]> Bootstrap5 <tibble [2 Ã— 4]> <tibble [0 Ã— 1]> <tibble [1 Ã—â€¦
```

ë¦¬ìƒ˜í”Œë§ ê²°ê³¼ì— `.extracts` ì—´ì´ ìƒê²¼ìŠµë‹ˆë‹¤.
ì´ ê°ì²´ì—ëŠ” ê° ë¦¬ìƒ˜í”Œì— ëŒ€í•œ `get_lm_coefs()` ì•„ì›ƒí’‹ì´ ìˆìŠµë‹ˆë‹¤.
ì´ `.extracts` ì—´ êµ¬ì¡°ëŠ” ì¡°ê¸ˆ ë³µì¡í•©ë‹ˆë‹¤.
ì²«ë²ˆì§¸ ìš”ì†Œ (ì²«ë²ˆì§¸ ë¦¬ìƒ˜í”Œì— í•´ë‹¹) ë¥¼ ë³´ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ì‹œë‹¤:


```r
lm_res$.extracts[[1]]
#> # A tibble: 1 Ã— 2
#>   .extracts        .config             
#>   <list>           <chr>               
#> 1 <tibble [4 Ã— 5]> Preprocessor1_Model1
```

ì´ ìš”ì†Œì—ëŠ” `tidy()` í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ë¥¼ ê°€ì§„ `.extracts` ì´ë¦„ì˜ _ë˜ë‹¤ë¥¸_ ì—´ì´ ìˆìŠµë‹ˆë‹¤:


```r
lm_res$.extracts[[1]]$.extracts[[1]]
#> # A tibble: 4 Ã— 5
#>   term        estimate std.error statistic   p.value
#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>
#> 1 (Intercept)    1.40     0.157       8.90 7.23e- 19
#> 2 Clark_Lake     0.842    0.0280     30.1  2.39e-184
#> 3 Austin         1.46     0.320       4.54 5.70e-  6
#> 4 Harlem        -0.637    0.163      -3.92 9.01e-  5
```

ì´ëŸ¬í•œ ì¤‘ì²©ëœ ì—´ë“¤ì€ purrr `unnest()` í•¨ìˆ˜ë¥¼ í†µí•´ flat í•˜ê²Œ ë§Œë“¤ìˆ˜ ìˆìŠµë‹ˆë‹¤: 


```r
lm_res %>% 
  select(id, .extracts) %>% 
  unnest(.extracts) 
#> # A tibble: 5 Ã— 3
#>   id         .extracts        .config             
#>   <chr>      <list>           <chr>               
#> 1 Bootstrap1 <tibble [4 Ã— 5]> Preprocessor1_Model1
#> 2 Bootstrap2 <tibble [4 Ã— 5]> Preprocessor1_Model1
#> 3 Bootstrap3 <tibble [4 Ã— 5]> Preprocessor1_Model1
#> 4 Bootstrap4 <tibble [4 Ã— 5]> Preprocessor1_Model1
#> 5 Bootstrap5 <tibble [4 Ã— 5]> Preprocessor1_Model1
```

ì¤‘ì²©ëœ í‹°ë¸” ì—´ì´ ì—¬ì „íˆ ë‚¨ì•„ìˆê¸° ë•Œë¬¸ì—, ë°ì´í„°ë¥¼ ìœ ìš©í•œ í¬ë§·ìœ¼ë¡œ ë§Œë“œëŠ” ê°™ì€ ëª…ë ¹ì–´ë¥¼ ë‹¤ì‹œ ìˆ˜í–‰í•©ë‹ˆë‹¤:


```r
lm_coefs <- 
  lm_res %>% 
  select(id, .extracts) %>% 
  unnest(.extracts) %>% 
  unnest(.extracts)

lm_coefs %>% select(id, term, estimate, p.value)
#> # A tibble: 20 Ã— 4
#>    id         term        estimate   p.value
#>    <chr>      <chr>          <dbl>     <dbl>
#>  1 Bootstrap1 (Intercept)    1.40  7.23e- 19
#>  2 Bootstrap1 Clark_Lake     0.842 2.39e-184
#>  3 Bootstrap1 Austin         1.46  5.70e-  6
#>  4 Bootstrap1 Harlem        -0.637 9.01e-  5
#>  5 Bootstrap2 (Intercept)    1.69  2.87e- 28
#>  6 Bootstrap2 Clark_Lake     0.911 1.06e-219
#>  7 Bootstrap2 Austin         0.595 5.93e-  2
#>  8 Bootstrap2 Harlem        -0.580 3.88e-  4
#>  9 Bootstrap3 (Intercept)    1.27  3.43e- 16
#> 10 Bootstrap3 Clark_Lake     0.859 5.03e-194
#> 11 Bootstrap3 Austin         1.09  6.77e-  4
#> 12 Bootstrap3 Harlem        -0.470 4.34e-  3
#> 13 Bootstrap4 (Intercept)    1.95  2.91e- 34
#> 14 Bootstrap4 Clark_Lake     0.974 1.47e-233
#> 15 Bootstrap4 Austin        -0.116 7.21e-  1
#> 16 Bootstrap4 Harlem        -0.620 2.11e-  4
#> 17 Bootstrap5 (Intercept)    1.87  1.98e- 33
#> 18 Bootstrap5 Clark_Lake     0.901 1.16e-210
#> 19 Bootstrap5 Austin         0.494 1.15e-  1
#> 20 Bootstrap5 Harlem        -0.512 1.73e-  3
```

ë” ë‚˜ì•„ì¡ŒìŠµë‹ˆë‹¤!
ì´ì œ, ê° ë¦¬ìƒ˜í”Œì˜ ëª¨ë¸ ê³„ìˆ˜ë¥¼ í”Œë¡¯í•´ë´…ì‹œë‹¤.


```r
lm_coefs %>%
  filter(term != "(Intercept)") %>% 
  ggplot(aes(x = term, y = estimate, group = id, col = id)) +  
  geom_hline(yintercept = 0, lty = 3) + 
  geom_line(alpha = 0.3, lwd = 1.2) + 
  labs(y = "Coefficient", x = NULL) +
  theme(legend.position = "top")
```

<img src="figs/lm-plot-1.svg" width="672" />

Austin ì—­ ë°ì´í„°ì˜ ê³„ìˆ˜ì— ìˆì–´ì„œ uncertainty ê°€ í¬ê³ , ë‹¤ë¥¸ ë‘ ì—­ì— ëŒ€í•´ì„œëŠ” ì‘ì€ ê²ƒ ê°™ì´ ë³´ì…ë‹ˆë‹¤.
ê²°ê³¼ë¥¼ unnest í•˜ëŠ” ì½”ë“œë¥¼ ë³´ë©´, double-nesting êµ¬ì¡°ê°€ ê³¼í•˜ê±°ë‚˜ ê·€ì°®ì„ ê²ƒì…ë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜, ì¶”ì¶œ ê¸°ëŠ¥ì€ ìœ ì—°ì„±ì´ ìˆê³ , ë” ê°„ë‹¨í•œ êµ¬ì¡°ë¡œëŠ” ë§ì€ use case ë¥¼ í•  ìˆ˜ ì—†ì—ˆì„ ê²ƒì…ë‹ˆë‹¤.

## ë³µì¡í•œ ëª¨ë¸: glmnet

glmnet ëª¨ë¸ì€ ìœ„ì—ì„œ ë³¸ ê²ƒê³¼ ê°™ì€ ì„ í˜• íšŒê·€ëª¨í˜•ì„ ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ ëª¨ë¸ì€ regulization (a.k.a penalization) ì„ ì‚¬ìš©í•˜ì—¬  ëª¨ë¸ íŒŒë¼í‚¤í„°ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
ì´ë ‡ê²Œ í•˜ë©´ ê³„ìˆ˜ë¥¼ 0 ìœ¼ë¡œ ì¶•ì†Œì‹œí‚¤ëŠ”ë°, ì„¤ëª…ë³€ìˆ˜ ì‚¬ì´ì— ìƒê´€ì„±ì´ í¬ê±°ë‚˜, ë³€ìˆ˜ ì„ íƒì´ í•„ìš”í•  ë•Œ ì¤‘ìš”í•©ë‹ˆë‹¤. 
ìš°ë¦¬ Chiacago ì—´ì°¨ë°ì´í„°ì…‹ì— ë‘ ê²½ìš° ë‹¤ í•´ë‹¹í•©ë‹ˆë‹¤. 

ì´ ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” ë‘ ê°€ì§€ ìœ í˜•ì˜ penalization ì´ ìˆìŠµë‹ˆë‹¤:

* Lasso (a.k.a. `\(L_1\)`) íŒ¨ë„í‹°ëŠ” ì ˆëŒ€ê°’ 0 ì´ ë  ì •ë„ë¡œ ëª¨ë¸ í•­ì„ ì¶•ì†Œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì¦‰, í•´ë‹¹ íš¨ê³¼ê°€ ëª¨ë¸ì—ì„œ ì™„ì „íˆ ì œê±°ë¨). 

* Weight decay (a.k.a ridge íšŒê·€ í˜¹ì€ `\(L_2\)`) ëŠ” ìƒê´€ì„±ì´ ê°•í•œ ì„¤ëª…ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ê°€ì¥ íš¨ê³¼ì ì¸ ìœ í˜•ì˜ íŒ¨ë„í‹°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 

glmnet ëª¨ë¸ì€ ë‘ ê°€ì§€ì˜ íŠœë‹íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ”ë°, penalization ì „ì²´ ì–‘ê³¼ ë‘ íŒ¨ë„í‹° ìœ í˜•ì˜ mixture ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì´ specification ì€:


```r
glmnet_spec <- 
  linear_reg(penalty = 0.1, mixture = 0.95) %>% 
  set_engine("glmnet")
```

95% lasso ì™€ 5% weight decay ì¸ íŒ¨ë„í‹°ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ ë‘ íŒ¨ë„í‹°ì˜ ì „ì²´ ì–‘ì€ 0.1 (ìƒë‹¹íˆ ë†’ì€ ê°’) ì…ë‹ˆë‹¤. 

{{% note %}} Models with regularization require that predictors are all on the same scale. The ridership at our three stations are very different, but glmnet [automatically centers and scales the data](https://parsnip.tidymodels.org/reference/details_linear_reg_glmnet.html). You can use recipes to [center and scale your data yourself](https://recipes.tidymodels.org/reference/step_normalize.html). {{%/ note %}}

ëª¨ë¸ specification ê³¼ ëª¨ë¸ `workflow()` ì˜ ê³µì‹ì„ ê²°í•©í•œ ë’¤ ëª¨ë¸ì„ ë°ì´í„°ì— ì í•©í•´ ë´…ì‹œë‹¤:


```r
glmnet_wflow <- 
  workflow() %>% 
  add_model(glmnet_spec) %>% 
  add_formula(ridership ~ .)

glmnet_fit <- fit(glmnet_wflow, Chicago)
glmnet_fit
#> â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#> Preprocessor: Formula
#> Model: linear_reg()
#> 
#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#> ridership ~ .
#> 
#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#> 
#> Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = "gaussian",      alpha = ~0.95) 
#> 
#>    Df %Dev Lambda
#> 1   0  0.0   6.10
#> 2   1 12.8   5.56
#> 3   1 23.4   5.07
#> 4   1 32.4   4.62
#> 5   1 40.0   4.21
#> 6   1 46.2   3.83
#> 7   1 51.5   3.49
#> 8   1 55.9   3.18
#> 9   1 59.6   2.90
#> 10  1 62.7   2.64
#> 11  2 65.3   2.41
#> 12  2 67.4   2.19
#> 13  2 69.2   2.00
#> 14  2 70.7   1.82
#> 15  2 72.0   1.66
#> 16  2 73.0   1.51
#> 17  2 73.9   1.38
#> 18  2 74.6   1.26
#> 19  2 75.2   1.14
#> 20  2 75.7   1.04
#> 21  2 76.1   0.95
#> 22  2 76.4   0.86
#> 23  2 76.7   0.79
#> 24  2 76.9   0.72
#> 25  2 77.1   0.66
#> 26  2 77.3   0.60
#> 27  2 77.4   0.54
#> 28  2 77.6   0.50
#> 29  2 77.6   0.45
#> 30  2 77.7   0.41
#> 31  2 77.8   0.38
#> 32  2 77.8   0.34
#> 33  2 77.9   0.31
#> 34  2 77.9   0.28
#> 35  2 78.0   0.26
#> 36  2 78.0   0.23
#> 37  2 78.0   0.21
#> 38  2 78.0   0.20
#> 39  2 78.0   0.18
#> 40  2 78.0   0.16
#> 41  2 78.0   0.15
#> 42  2 78.1   0.14
#> 43  2 78.1   0.12
#> 44  2 78.1   0.11
#> 45  2 78.1   0.10
#> 46  2 78.1   0.09
#> 
#> ...
#> and 9 more lines.
```

ì´ ì•„ì›ƒí’‹ì—ì„œ, `lambda` í•­ì€ íŒ¨ë„í‹°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

`penalty = 0.1` specification ì—ë„ ë¶ˆêµ¬í•˜ê³  ì•„ì›ƒí’‹ì—ì„œ íŒ¨ë„í‹°ì˜ ì—¬ëŸ¬ ê°’ì´ ì¶œë ¥ë˜ì—ˆìŠµë‹ˆë‹¤. íŒ¨ë„í‹° ê°’ "path" ì— ì í•©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 0.1 ê°’ì— ê´€ì‹¬ì´ ìˆë”ë¼ë„, ê°™ì€ ëª¨ë¸ ê°ì²´ì˜ ì—¬ëŸ¬ íŒ¨ë„í‹° ê°’ì— ëŒ€í•œ ëª¨ë¸ ê³„ìˆ˜ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê³„ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ë‘ê°€ì§€ ë‹¤ë¥¸ ë°©ë²•ì„ ì‚´í´ë´…ì‹œë‹¤. ë‘ ë°©ë²• ë‹¤ `tidy()` ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•œ ë°©ë²•ì€ glmnet ê°ì²´ë¥¼ íƒ€ì´ë””í•˜ê²Œ í•˜ê³  ë‹¤ë¥¸ ë°©ë²•ì€, tidymodels ê°ì²´ë¥¼ íƒ€ì´ë””í•˜ê²Œ í•  ê²ƒì…ë‹ˆë‹¤.

### glmnet íŒ¨ë„í‹° ê°’ì„ ì‚¬ìš©

ì´ glmnet fit ì—ëŠ” ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ëŠ” ì—¬ëŸ¬ íŒ¨ë„í‹° ê°’ì´ ìˆìŠµë‹ˆë‹¤;  
ë°ì´í„°(í˜¹ì€ mixture ì–‘)ë¥¼ ë°”ê¾¸ë©´ ë‹¤ë¥¸ íŒ¨ë„í‹°ê°’ì´ ì‚°ì¶œë©ë‹ˆë‹¤. 
ì´ ë°ì´í„°ì…‹ì—ëŠ”, 55 ê°œì˜ íŒ¨ë„í‹°ê°€ ìˆìŠµë‹ˆë‹¤. 
ì´ ë°ì´í„°ì…‹ì—ì„œ ì‚°ì¶œëœ íŒ¨ë„í‹°ë¥¼ êµ¬í•˜ê¸° ìœ„í•´, ì—”ì§„ fit ì„ ì¶”ì¶œí•˜ê³ , íƒ€ì´ë””í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```r
glmnet_fit %>% 
  extract_fit_engine() %>% 
  tidy() %>% 
  rename(penalty = lambda) %>%   # <- for consistent naming
  filter(term != "(Intercept)")
#> # A tibble: 99 Ã— 5
#>    term        step estimate penalty dev.ratio
#>    <chr>      <dbl>    <dbl>   <dbl>     <dbl>
#>  1 Clark_Lake     2   0.0753    5.56     0.127
#>  2 Clark_Lake     3   0.145     5.07     0.234
#>  3 Clark_Lake     4   0.208     4.62     0.324
#>  4 Clark_Lake     5   0.266     4.21     0.400
#>  5 Clark_Lake     6   0.319     3.83     0.463
#>  6 Clark_Lake     7   0.368     3.49     0.515
#>  7 Clark_Lake     8   0.413     3.18     0.559
#>  8 Clark_Lake     9   0.454     2.90     0.596
#>  9 Clark_Lake    10   0.491     2.64     0.627
#> 10 Clark_Lake    11   0.526     2.41     0.653
#> # â€¦ with 89 more rows
```

ì¶œë ¥ëœ ê²ƒì„ ë³´ë©´, ì˜ ë™ì‘í•œ ê²ƒ ê°™ì§€ë§Œ, ìš°ë¦¬ íŒ¨ë„í‹° ê°’ (0.1) ì´ ëª¨ë¸ì—ì„œ ì‚°ì¶œí•œ ëª©ë¡ì— ì—†ìŠµë‹ˆë‹¤!
ë‚´ë¶€ íŒ¨í‚¤ì§€ì—ëŠ” interpolation ì„ ì´ìš©í•˜ì—¬, ì´ êµ¬ì²´ì  ê°’ì— í•´ë‹¹í•˜ëŠ” ê³„ìˆ˜ë¥¼ ì‚°ì¶œí•˜ëŠ” í•¨ìˆ˜ë“¤ì´ ìˆì§€ë§Œ, glmnet ê°ì²´ì— ëŒ€í•œ `tidy()` ë©”ì†Œë“œëŠ” ì´ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 

### íŠ¹ì • íŒ¨ë„í‹° ê°’ ì‚¬ìš©í•˜ê¸°

`tidy()` ë©”ì†Œë“œë¥¼ ì›Œí¬í”Œë¡œë‚˜ parsnip ê°ì²´ì— ì‹¤í–‰í•œë‹¤ë©´, ìš°ë¦¬ê°€ íŠ¹ì •í•œ íŒ¨ë„í‹° ê°’ì— í•´ë‹¹í•˜ëŠ” ê³„ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” ë‹¤ë¥¸ í•¨ìˆ˜ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤: 


```r
tidy(glmnet_fit)
#> # A tibble: 4 Ã— 3
#>   term        estimate penalty
#>   <chr>          <dbl>   <dbl>
#> 1 (Intercept)    1.69      0.1
#> 2 Clark_Lake     0.846     0.1
#> 3 Austin         0.271     0.1
#> 4 Harlem         0         0.1
```

ë‹¤ë¥¸ (single) íŒ¨ë„í‹°ì— ëŒ€í•´, ì¶”ê°€ ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```r
tidy(glmnet_fit, penalty = 5.5620)  # A value from above
#> # A tibble: 4 Ã— 3
#>   term        estimate penalty
#>   <chr>          <dbl>   <dbl>
#> 1 (Intercept)  12.6       5.56
#> 2 Clark_Lake    0.0753    5.56
#> 3 Austin        0         5.56
#> 4 Harlem        0         5.56
```

ë‘ ê°œì˜ `tidy()` ë©”ì†Œë“œê°€ ìˆëŠ” ì´ìœ ëŠ” tidymodels ì—ì„œì˜ ì£¼ì•ˆì ì€ íŠ¹ì •í•œ íŒ¨ë„í‹° ê°’ì— ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 


### glmnet ëª¨ë¸ íŠœë‹í•˜ê¸°

If we know a priori acceptable values for penalty and mixture, we can use the `fit_resamples()` function as we did before with linear regression. Otherwise, we can tune those parameters with the tidymodels `tune_*()` functions. 

Let's tune our glmnet model over both parameters with this grid: 


```r
pen_vals <- 10^seq(-3, 0, length.out = 10)
grid <- crossing(penalty = pen_vals, mixture = c(0.1, 1.0))
```

Here is where more glmnet-related complexity comes in: we know that each resample and each value of `mixture` will probably produce a different set of penalty values contained in the model object. _How can we look at the coefficients at the specific penalty values that we are using to tune?_

The approach that we suggest is to use the special `path_values` option for glmnet. Details are described in the [technical documentation about glmnet and tidymodels](https://parsnip.tidymodels.org/reference/glmnet-details.html#arguments) but in short, this parameter will assign the collection of penalty values used by each glmnet fit (regardless of the data or value of mixture). 

We can pass these as an engine argument and then update our previous workflow object:


```r
glmnet_tune_spec <- 
  linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet", path_values = pen_vals)

glmnet_wflow <- 
  glmnet_wflow %>% 
  update_model(glmnet_tune_spec)
```

Now we will use an extraction function similar to when we used ordinary least squares. We add an additional argument to retain coefficients that are shrunk to zero by the lasso penalty: 


```r
get_glmnet_coefs <- function(x) {
  x %>% 
    extract_fit_engine() %>% 
    tidy(return_zeros = TRUE) %>% 
    rename(penalty = lambda)
}
parsnip_ctrl <- control_grid(extract = get_glmnet_coefs)

glmnet_res <- 
  glmnet_wflow %>% 
  tune_grid(
    resamples = bt,
    grid = grid,
    control = parsnip_ctrl
  )
glmnet_res
#> # Tuning results
#> # Bootstrap sampling 
#> # A tibble: 5 Ã— 5
#>   splits              id         .metrics          .notes           .extracts   
#>   <list>              <chr>      <list>            <list>           <list>      
#> 1 <split [5698/2076]> Bootstrap1 <tibble [40 Ã— 6]> <tibble [0 Ã— 1]> <tibble [20â€¦
#> 2 <split [5698/2098]> Bootstrap2 <tibble [40 Ã— 6]> <tibble [0 Ã— 1]> <tibble [20â€¦
#> 3 <split [5698/2064]> Bootstrap3 <tibble [40 Ã— 6]> <tibble [0 Ã— 1]> <tibble [20â€¦
#> 4 <split [5698/2082]> Bootstrap4 <tibble [40 Ã— 6]> <tibble [0 Ã— 1]> <tibble [20â€¦
#> 5 <split [5698/2088]> Bootstrap5 <tibble [40 Ã— 6]> <tibble [0 Ã— 1]> <tibble [20â€¦
```

As noted before, the elements of the main `.extracts` column have an embedded list column with the results of `get_glmnet_coefs()`:  


```r
glmnet_res$.extracts[[1]] %>% head()
#> # A tibble: 6 Ã— 4
#>   penalty mixture .extracts         .config              
#>     <dbl>   <dbl> <list>            <chr>                
#> 1       1     0.1 <tibble [40 Ã— 5]> Preprocessor1_Model01
#> 2       1     0.1 <tibble [40 Ã— 5]> Preprocessor1_Model02
#> 3       1     0.1 <tibble [40 Ã— 5]> Preprocessor1_Model03
#> 4       1     0.1 <tibble [40 Ã— 5]> Preprocessor1_Model04
#> 5       1     0.1 <tibble [40 Ã— 5]> Preprocessor1_Model05
#> 6       1     0.1 <tibble [40 Ã— 5]> Preprocessor1_Model06

glmnet_res$.extracts[[1]]$.extracts[[1]] %>% head()
#> # A tibble: 6 Ã— 5
#>   term         step estimate penalty dev.ratio
#>   <chr>       <dbl>    <dbl>   <dbl>     <dbl>
#> 1 (Intercept)     1    0.568  1          0.769
#> 2 (Intercept)     2    0.432  0.464      0.775
#> 3 (Intercept)     3    0.607  0.215      0.779
#> 4 (Intercept)     4    0.846  0.1        0.781
#> 5 (Intercept)     5    1.06   0.0464     0.782
#> 6 (Intercept)     6    1.22   0.0215     0.783
```

As before, we'll have to use a double `unnest()`. Since the penalty value is in both the top-level and lower-level `.extracts`, we'll use `select()` to get rid of the first version (but keep `mixture`):


```r
glmnet_res %>% 
  select(id, .extracts) %>% 
  unnest(.extracts) %>% 
  select(id, mixture, .extracts) %>%  # <- removes the first penalty column
  unnest(.extracts)
```

But wait! We know that each glmnet fit contains all of the coefficients. This means, for a specific resample and value of `mixture`, the results are the same:  


```r
all.equal(
  # First bootstrap, first `mixture`, first `penalty`
  glmnet_res$.extracts[[1]]$.extracts[[1]],
  # First bootstrap, first `mixture`, second `penalty`
  glmnet_res$.extracts[[1]]$.extracts[[2]]
)
#> [1] TRUE
```

For this reason, we'll add a `slice(1)` when grouping by `id` and `mixture`. This will get rid of the replicated results. 


```r
glmnet_coefs <- 
  glmnet_res %>% 
  select(id, .extracts) %>% 
  unnest(.extracts) %>% 
  select(id, mixture, .extracts) %>% 
  group_by(id, mixture) %>%          # â”
  slice(1) %>%                       # â”‚ Remove the redundant results
  ungroup() %>%                      # â”˜
  unnest(.extracts)

glmnet_coefs %>% 
  select(id, penalty, mixture, term, estimate) %>% 
  filter(term != "(Intercept)")
#> # A tibble: 300 Ã— 5
#>    id         penalty mixture term       estimate
#>    <chr>        <dbl>   <dbl> <chr>         <dbl>
#>  1 Bootstrap1 1           0.1 Clark_Lake    0.391
#>  2 Bootstrap1 0.464       0.1 Clark_Lake    0.485
#>  3 Bootstrap1 0.215       0.1 Clark_Lake    0.590
#>  4 Bootstrap1 0.1         0.1 Clark_Lake    0.680
#>  5 Bootstrap1 0.0464      0.1 Clark_Lake    0.746
#>  6 Bootstrap1 0.0215      0.1 Clark_Lake    0.793
#>  7 Bootstrap1 0.01        0.1 Clark_Lake    0.817
#>  8 Bootstrap1 0.00464     0.1 Clark_Lake    0.828
#>  9 Bootstrap1 0.00215     0.1 Clark_Lake    0.834
#> 10 Bootstrap1 0.001       0.1 Clark_Lake    0.837
#> # â€¦ with 290 more rows
```

Now we have the coefficients. Let's look at how they behave as more regularization is used: 


```r
glmnet_coefs %>% 
  filter(term != "(Intercept)") %>% 
  mutate(mixture = format(mixture)) %>% 
  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) + 
  geom_hline(yintercept = 0, lty = 3) +
  geom_line(alpha = 0.5, lwd = 1.2) + 
  facet_wrap(~ term) + 
  scale_x_log10() +
  scale_color_brewer(palette = "Accent") +
  labs(y = "coefficient") +
  theme(legend.position = "top")
```

<img src="figs/glmnet-plot-1.svg" width="816" />

Notice a couple of things: 

* With a pure lasso model (i.e., `mixture = 1`), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases. Also, as the penalty increases, the uncertainty in this coefficient decreases. 

* The Harlem predictor is either quickly selected out of the model or goes from negative to positive. 

## ì„¸ì…˜ì •ë³´


```
#> â”€ Session info  ğŸ†  ğŸ‘«ğŸ¼  ğŸ¦™   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  hash: AB button (blood type), woman and man holding hands: medium-light skin tone, llama
#> 
#>  setting  value
#>  version  R version 4.1.2 (2021-11-01)
#>  os       macOS Big Sur 10.16
#>  system   x86_64, darwin17.0
#>  ui       X11
#>  language (EN)
#>  collate  en_US.UTF-8
#>  ctype    en_US.UTF-8
#>  tz       Asia/Seoul
#>  date     2022-01-30
#>  pandoc   2.11.4 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)
#> 
#> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  package    * version date (UTC) lib source
#>  broom      * 0.7.11  2022-01-03 [1] CRAN (R 4.1.2)
#>  dials      * 0.0.10  2021-09-10 [1] CRAN (R 4.1.0)
#>  dplyr      * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)
#>  ggplot2    * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)
#>  glmnet     * 4.1-3   2021-11-02 [1] CRAN (R 4.1.0)
#>  infer      * 1.0.0   2021-08-13 [1] CRAN (R 4.1.0)
#>  parsnip    * 0.1.7   2021-07-21 [1] CRAN (R 4.1.0)
#>  purrr      * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)
#>  recipes    * 0.1.17  2021-09-27 [1] CRAN (R 4.1.0)
#>  rlang      * 0.4.12  2021-10-18 [1] CRAN (R 4.1.0)
#>  rsample    * 0.1.1   2021-11-08 [1] CRAN (R 4.1.0)
#>  tibble     * 3.1.6   2021-11-07 [1] CRAN (R 4.1.0)
#>  tidymodels * 0.1.4   2021-10-01 [1] CRAN (R 4.1.0)
#>  tune       * 0.1.6   2021-07-21 [1] CRAN (R 4.1.0)
#>  workflows  * 0.2.4   2021-10-12 [1] CRAN (R 4.1.0)
#>  yardstick  * 0.0.9   2021-11-22 [1] CRAN (R 4.1.0)
#> 
#>  [1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library
#> 
#> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```
