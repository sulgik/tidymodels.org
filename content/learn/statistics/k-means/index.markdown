---
title: "K-means clustering with tidy data principles"
tags: [broom]
categories: [statistical analysis]
type: learn-subsection
weight: 2
description: | 
  Summarize clustering characteristics and estimate the best number of clusters for a data set.
---





## Introduction

This article only requires the tidymodels package.

K-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions: 

- `tidy()`
- `augment()` 
- `glance()`

Let's start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster:


```r
library(tidymodels)

set.seed(27)

centers <- tibble(
  cluster = factor(1:3), 
  num_points = c(100, 150, 50),  # number points in each cluster
  x1 = c(5, 0, -3),              # x1 coordinate of cluster center
  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center
)

labelled_points <- 
  centers %>%
  mutate(
    x1 = map2(num_points, x1, rnorm),
    x2 = map2(num_points, x2, rnorm)
  ) %>% 
  select(-num_points) %>% 
  unnest(cols = c(x1, x2))

ggplot(labelled_points, aes(x1, x2, color = cluster)) +
  geom_point(alpha = 0.3)
```

<img src="figs/unnamed-chunk-1-1.svg" width="672" />

k-means í¬ëŸ¬ìŠ¤í„°ë§ì„ í•˜ê¸° ì´ìƒì ì¸ ì¼€ì´ìŠ¤ì…ë‹ˆë‹¤.

## K-means ëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?

ê³µì‹ì„ ì‚¬ìš©í•˜ê¸°ë³´ë‹¤, Allison Horst ì˜ [artwork](https://github.com/allisonhorst/stats-illustrations) ë¥¼ ì´ìš©í•œ ì´ ì§§ì€ ì• ë‹ˆë©”ì´ì…˜ì€ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤:

<img src="kmeans.gif" style="display: block; margin: auto;" />

## R ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§


ë©”ì¸ ì…ë ¥ì¸ìˆ˜ê°€ ëª¨ë“  ì»¬ëŸ¼ì´ ìˆ˜ì¹˜í˜•ì¸ ë°ì´í„°í”„ë ˆì„ì¸ ë¹ŒíŠ¸ì¸ `kmeans()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.


```r
points <- 
  labelled_points %>% 
  select(-cluster)

kclust <- kmeans(points, centers = 3)
kclust
#> K-means clustering with 3 clusters of sizes 148, 51, 101
#> 
#> Cluster means:
#>        x1    x2
#> 1  0.0885  1.05
#> 2 -3.1429 -2.00
#> 3  5.0040 -1.05
#> 
#> Clustering vector:
#>   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
#>  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
#>  [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1
#> [112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#> [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#> [186] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#> [223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2
#> [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
#> [297] 2 2 2 2
#> 
#> Within cluster sum of squares by cluster:
#> [1] 299 109 243
#>  (between_SS / total_SS =  82.5 %)
#> 
#> Available components:
#> 
#> [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
#> [6] "betweenss"    "size"         "iter"         "ifault"
summary(kclust)
#>              Length Class  Mode   
#> cluster      300    -none- numeric
#> centers        6    -none- numeric
#> totss          1    -none- numeric
#> withinss       3    -none- numeric
#> tot.withinss   1    -none- numeric
#> betweenss      1    -none- numeric
#> size           3    -none- numeric
#> iter           1    -none- numeric
#> ifault         1    -none- numeric
```

ì¶œë ¥ì€ ê¸¸ì´ê°€ ë‹¤ë¥¸ ìš”ì†Œë“¤ì˜ ë²¡í„°ë“¤ë¡œ ì´ë£¨ì–´ì§„ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
ì› ë°ì´í„°ì…‹ê³¼ ê°™ì€ ê¸¸ì´ê°€ 300 ì¸ ê²ƒì´ í•˜ë‚˜ ìˆìŠµë‹ˆë‹¤.
ê¸¸ì´ê°€ 3 ì¸ ë‘ ìš”ì†Œ (`withinss` and `tot.withinss`) ê°€ ìˆê³ , `centers` ëŠ” í–‰ì´ 3 ì¸ í–‰ë ¬ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ ê¸¸ì´ê°€ 1 ì¸ ìš”ì†Œë“¤ì´ ìˆìŠµë‹ˆë‹¤: `totss`, `tot.withinss`, `betweenss`, `iter`. (`ifault` ê°’ì€ ê°€ëŠ¥í•­ ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œë“¤ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.)

ìš°ë¦¬ ë°ì´í„°ì…‹ì„ íƒ€ì´ë””í•˜ê²Œ í•˜ê³  ì‹¶ì„ ë•Œ ì´ ë‹¤ë¥¸ ê¸¸ì´ë“¤ì€ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°–ìŠµë‹ˆë‹¤; ê·¸ë“¤ì€ ê° ìœ í˜•ì˜ êµ¬ì„±ìš”ì†Œë“¤ì´ *ë‹¤ë¥¸ ì¢…ë¥˜* ì˜ ì •ë³´ë¥¼ ì†Œí†µí•¨ì„ ìƒì§•í•©ë‹ˆë‹¤.

- `cluster` (300 ê°œì˜ ê°’ë“¤) ëŠ” ê° *ì * ì— ê´€í•œ ì •ë³´ê°€ ìˆìŠµë‹ˆë‹¤
- `centers`, `withinss`, `size` (3 values) ëŠ” ê° *í´ëŸ¬ìŠ¤í„°* ì— ê´€í•œ ì •ë³´ê°€ ìˆìŠµë‹ˆë‹¤
- `totss`, `tot.withinss`, `betweenss`, `iter` (1 ê°’) ì—ëŠ” *full clustering* ì— ê´€í•œ ì •ë³´ê°€ ìˆìŠµë‹ˆë‹¤

Which of these do we want to extract? There is no right answer; each of them may be interesting to an analyst. Because they communicate entirely different information (not to mention there's no straightforward way to combine them), they are extracted by separate functions. `augment` adds the point classifications to the original data set:


```r
augment(kclust, points)
#> # A tibble: 300 Ã— 3
#>       x1     x2 .cluster
#>    <dbl>  <dbl> <fct>   
#>  1  6.91 -2.74  3       
#>  2  6.14 -2.45  3       
#>  3  4.24 -0.946 3       
#>  4  3.54  0.287 3       
#>  5  3.91  0.408 3       
#>  6  5.30 -1.58  3       
#>  7  5.01 -1.77  3       
#>  8  6.16 -1.68  3       
#>  9  7.13 -2.17  3       
#> 10  5.24 -2.42  3       
#> # â€¦ with 290 more rows
```

`tidy()` í•¨ìˆ˜ëŠ” í´ëŸ¬ìŠ¤í„°ë³„ ìˆ˜ì¤€ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤:


```r
tidy(kclust)
#> # A tibble: 3 Ã— 5
#>        x1    x2  size withinss cluster
#>     <dbl> <dbl> <int>    <dbl> <fct>  
#> 1  0.0885  1.05   148     299. 1      
#> 2 -3.14   -2.00    51     109. 2      
#> 3  5.00   -1.05   101     243. 3
```

And as it always does, the `glance()` function extracts a single-row summary:


```r
glance(kclust)
#> # A tibble: 1 Ã— 4
#>   totss tot.withinss betweenss  iter
#>   <dbl>        <dbl>     <dbl> <int>
#> 1 3724.         651.     3073.     2
```

## Exploratory clustering

While these summaries are useful, they would not have been too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like [dplyr](https://dplyr.tidyverse.org/).

Let's say we want to explore the effect of different choices of `k`, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of `k`, then create columns containing the tidied, glanced and augmented data:


```r
kclusts <- 
  tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(points, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )

kclusts
#> # A tibble: 9 Ã— 5
#>       k kclust   tidied           glanced          augmented         
#>   <int> <list>   <list>           <list>           <list>            
#> 1     1 <kmeans> <tibble [1 Ã— 5]> <tibble [1 Ã— 4]> <tibble [300 Ã— 3]>
#> 2     2 <kmeans> <tibble [2 Ã— 5]> <tibble [1 Ã— 4]> <tibble [300 Ã— 3]>
#> 3     3 <kmeans> <tibble [3 Ã— 5]> <tibble [1 Ã— 4]> <tibble [300 Ã— 3]>
#> 4     4 <kmeans> <tibble [4 Ã— 5]> <tibble [1 Ã— 4]> <tibble [300 Ã— 3]>
#> 5     5 <kmeans> <tibble [5 Ã— 5]> <tibble [1 Ã— 4]> <tibble [300 Ã— 3]>
#> 6     6 <kmeans> <tibble [6 Ã— 5]> <tibble [1 Ã— 4]> <tibble [300 Ã— 3]>
#> 7     7 <kmeans> <tibble [7 Ã— 5]> <tibble [1 Ã— 4]> <tibble [300 Ã— 3]>
#> 8     8 <kmeans> <tibble [8 Ã— 5]> <tibble [1 Ã— 4]> <tibble [300 Ã— 3]>
#> 9     9 <kmeans> <tibble [9 Ã— 5]> <tibble [1 Ã— 4]> <tibble [300 Ã— 3]>
```

We can turn these into three separate data sets each representing a different type of data: using `tidy()`, using `augment()`, and using `glance()`. Each of these goes into a separate data set as they represent different types of data.


```r
clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented))

clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))
```

Now we can plot the original points using the data from `augment()`, with each point colored according to the predicted cluster.


```r
p1 <- 
  ggplot(assignments, aes(x = x1, y = x2)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k)
p1
```

<img src="figs/unnamed-chunk-8-1.svg" width="672" />

Already we get a good sense of the proper number of clusters (3), and how the k-means algorithm functions when `k` is too high or too low. We can then add the centers of the cluster using the data from `tidy()`:


```r
p2 <- p1 + geom_point(data = clusters, size = 10, shape = "x")
p2
```

<img src="figs/unnamed-chunk-9-1.svg" width="672" />

The data from `glance()` fills a different but equally important purpose; it lets us view trends of some summary statistics across values of `k`. Of particular interest is the total within sum of squares, saved in the `tot.withinss` column.


```r
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()
```

<img src="figs/unnamed-chunk-10-1.svg" width="672" />

This represents the variance within the clusters. It decreases as `k` increases, but notice a bend (or "elbow") around `k = 3`. This bend indicates that additional clusters beyond the third have little value. (See [here](https://web.stanford.edu/~hastie/Papers/gap.pdf) for a more mathematically rigorous interpretation and implementation of this method). Thus, all three methods of tidying data provided by broom are useful for summarizing clustering output.

## Session information


```
#> â”€ Session info  ğŸ‘§ğŸ¼  â›±ï¸  ğŸ‡¸ğŸ‡·   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  hash: girl: medium-light skin tone, umbrella on ground, flag: Suriname
#> 
#>  setting  value
#>  version  R version 4.1.1 (2021-08-10)
#>  os       macOS Big Sur 10.16
#>  system   x86_64, darwin17.0
#>  ui       X11
#>  language (EN)
#>  collate  en_US.UTF-8
#>  ctype    en_US.UTF-8
#>  tz       Asia/Seoul
#>  date     2022-01-10
#>  pandoc   2.11.4 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)
#> 
#> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  package    * version date (UTC) lib source
#>  broom      * 0.7.10  2021-10-31 [1] CRAN (R 4.1.0)
#>  dials      * 0.0.10  2021-09-10 [1] CRAN (R 4.1.0)
#>  dplyr      * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)
#>  ggplot2    * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)
#>  infer      * 1.0.0   2021-08-13 [1] CRAN (R 4.1.0)
#>  parsnip    * 0.1.7   2021-07-21 [1] CRAN (R 4.1.0)
#>  purrr      * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)
#>  recipes    * 0.1.17  2021-09-27 [1] CRAN (R 4.1.0)
#>  rlang        0.4.12  2021-10-18 [1] CRAN (R 4.1.0)
#>  rsample    * 0.1.1   2021-11-08 [1] CRAN (R 4.1.0)
#>  tibble     * 3.1.6   2021-11-07 [1] CRAN (R 4.1.0)
#>  tidymodels * 0.1.4   2021-10-01 [1] CRAN (R 4.1.0)
#>  tune       * 0.1.6   2021-07-21 [1] CRAN (R 4.1.0)
#>  workflows  * 0.2.4   2021-10-12 [1] CRAN (R 4.1.0)
#>  yardstick  * 0.0.9   2021-11-22 [1] CRAN (R 4.1.0)
#> 
#>  [1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library
#> 
#> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

