---
title: "K-means clustering with tidy data principles"
tags: [broom]
categories: [statistical analysis]
type: learn-subsection
weight: 2
description: | 
  Summarize clustering characteristics and estimate the best number of clusters for a data set.
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
source(here::here("content/learn/common.R"))
```

```{r load, include = FALSE}
library(tidymodels)
pkgs <- c("tidymodels")

theme_set(theme_bw() + theme(legend.position = "top"))
```

## Introduction

This article only requires the tidymodels package.

K-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions: 

- `tidy()`
- `augment()` 
- `glance()`

Let's start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster:

```{r}
library(tidymodels)

set.seed(27)

centers <- tibble(
  cluster = factor(1:3), 
  num_points = c(100, 150, 50),  # number points in each cluster
  x1 = c(5, 0, -3),              # x1 coordinate of cluster center
  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center
)

labelled_points <- 
  centers %>%
  mutate(
    x1 = map2(num_points, x1, rnorm),
    x2 = map2(num_points, x2, rnorm)
  ) %>% 
  select(-num_points) %>% 
  unnest(cols = c(x1, x2))

ggplot(labelled_points, aes(x1, x2, color = cluster)) +
  geom_point(alpha = 0.3)
```

k-means 클러스터링을 하기 이상적인 케이스입니다.

## K-means 는 어떻게 작동하나요?

공식을 사용하기보다, Allison Horst 의 [artwork](https://github.com/allisonhorst/stats-illustrations) 를 이용한 이 짧은 애니메이션은 클러스터링 프로세스를 설명합니다:

```{r illustrations, echo = FALSE, results = 'asis', fig.align="center"}
knitr::include_graphics("kmeans.gif")
```

## R 에서 클러스터링


메인 입력인수가 모든 컬럼이 수치형인 데이터프레임인 빌트인 `kmeans()` 함수를 사용할 것입니다.

```{r}
points <- 
  labelled_points %>% 
  select(-cluster)

kclust <- kmeans(points, centers = 3)
kclust
summary(kclust)
```

출력은 길이가 다른 요소들의 벡터들로 이루어진 리스트입니다. 
원 데이터셋과 같은 길이가 `r nrow(points)` 인 것이 하나 있습니다.
길이가 3 인 두 요소 (`withinss` and `tot.withinss`) 가 있고, `centers` 는 행이 3 인 행렬입니다. 그리고 나서 길이가 1 인 요소들이 있습니다: `totss`, `tot.withinss`, `betweenss`, `iter`. (`ifault` 값은 가능항 알고리즘 문제들을 가리킵니다.)

우리 데이터셋을 타이디하게 하고 싶을 때 이 다른 길이들은 중요한 의미를 갖습니다; 그들은 각 유형의 구성요소들이 *다른 종류* 의 정보를 소통함을 상징합니다.

- `cluster` (`r nrow(points)` 개의 값들) 는 각 *점* 에 관한 정보가 있습니다
- `centers`, `withinss`, `size` (3 values) 는 각 *클러스터* 에 관한 정보가 있습니다
- `totss`, `tot.withinss`, `betweenss`, `iter` (1 값) 에는 *full clustering* 에 관한 정보가 있습니다

Which of these do we want to extract? There is no right answer; each of them may be interesting to an analyst. Because they communicate entirely different information (not to mention there's no straightforward way to combine them), they are extracted by separate functions. `augment` adds the point classifications to the original data set:

```{r}
augment(kclust, points)
```

`tidy()` 함수는 클러스터별 수준기반으로 요약합니다:

```{r}
tidy(kclust)
```

And as it always does, the `glance()` function extracts a single-row summary:

```{r}
glance(kclust)
```

## Exploratory clustering

While these summaries are useful, they would not have been too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like [dplyr](https://dplyr.tidyverse.org/).

Let's say we want to explore the effect of different choices of `k`, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of `k`, then create columns containing the tidied, glanced and augmented data:

```{r}
kclusts <- 
  tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(points, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )

kclusts
```

We can turn these into three separate data sets each representing a different type of data: using `tidy()`, using `augment()`, and using `glance()`. Each of these goes into a separate data set as they represent different types of data.

```{r}
clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented))

clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))
```

Now we can plot the original points using the data from `augment()`, with each point colored according to the predicted cluster.

```{r, fig.width = 7, fig.height = 7}
p1 <- 
  ggplot(assignments, aes(x = x1, y = x2)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k)
p1
```

Already we get a good sense of the proper number of clusters (3), and how the k-means algorithm functions when `k` is too high or too low. We can then add the centers of the cluster using the data from `tidy()`:

```{r}
p2 <- p1 + geom_point(data = clusters, size = 10, shape = "x")
p2
```

The data from `glance()` fills a different but equally important purpose; it lets us view trends of some summary statistics across values of `k`. Of particular interest is the total within sum of squares, saved in the `tot.withinss` column.

```{r}
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()
```

This represents the variance within the clusters. It decreases as `k` increases, but notice a bend (or "elbow") around `k = 3`. This bend indicates that additional clusters beyond the third have little value. (See [here](https://web.stanford.edu/~hastie/Papers/gap.pdf) for a more mathematically rigorous interpretation and implementation of this method). Thus, all three methods of tidying data provided by broom are useful for summarizing clustering output.

## Session information

```{r si, echo = FALSE}
small_session(pkgs)
```

