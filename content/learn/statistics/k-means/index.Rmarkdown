---
title: "타이디 데이터 원칙과 함께 K-means 클러스터링"
tags: [broom]
categories: [statistical analysis]
type: learn-subsection
weight: 2
description: | 
  Summarize clustering characteristics and estimate the best number of clusters for a data set.
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}
source(here::here("content/learn/common.R"))
```

```{r load, include = FALSE}
library(tidymodels)
pkgs <- c("tidymodels")

theme_set(theme_bw() + theme(legend.position = "top"))
```

## 들어가기

이 장은 tidymodels 패키지만 필요로 합니다.

K-means 클러스터링 통계 분석에 타이디 데이터 원칙들을 적용하는 유용한 예제로 볼 수 있습니다. 특별히 다음의 타이디하게 하는 함수들 사이에 차이점을 볼 수 있습니다: 

- `tidy()`
- `augment()` 
- `glance()`

세 클러스터를 이루는 랜덤 2차원 데이터를 생성하는 것부터 시작해봅시다. 각 클러스터의 데이터는 다른 평균을 가지는 다변량 가우시안 분포로부터 생성될 것입니다:

```{r}
library(tidymodels)

set.seed(27)

centers <- tibble(
  cluster = factor(1:3), 
  num_points = c(100, 150, 50),  # number points in each cluster
  x1 = c(5, 0, -3),              # x1 coordinate of cluster center
  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center
)

labelled_points <- 
  centers %>%
  mutate(
    x1 = map2(num_points, x1, rnorm),
    x2 = map2(num_points, x2, rnorm)
  ) %>% 
  select(-num_points) %>% 
  unnest(cols = c(x1, x2))

ggplot(labelled_points, aes(x1, x2, color = cluster)) +
  geom_point(alpha = 0.3)
```

k-means 클러스터링을 하기 이상적인 케이스입니다.

## K-means 는 어떻게 작동하나요?

공식을 사용하기보다, Allison Horst 의 [artwork](https://github.com/allisonhorst/stats-illustrations) 를 이용한 이 짧은 애니메이션은 클러스터링 프로세스를 설명합니다:

```{r illustrations, echo = FALSE, results = 'asis', fig.align="center"}
knitr::include_graphics("kmeans.gif")
```

## R 에서 클러스터링


메인 입력인수가 모든 컬럼이 수치형인 데이터프레임인 빌트인 `kmeans()` 함수를 사용할 것입니다.

```{r}
points <- 
  labelled_points %>% 
  select(-cluster)

kclust <- kmeans(points, centers = 3)
kclust
summary(kclust)
```

출력은 길이가 다른 요소들의 벡터들로 이루어진 리스트입니다. 
원 데이터셋과 같은 길이가 `r nrow(points)` 인 것이 하나 있습니다.
길이가 3 인 두 요소 (`withinss` and `tot.withinss`) 가 있고, `centers` 는 행이 3 인 행렬입니다. 그리고 나서 길이가 1 인 요소들이 있습니다: `totss`, `tot.withinss`, `betweenss`, `iter`. (`ifault` 값은 가능항 알고리즘 문제들을 가리킵니다.)

우리 데이터셋을 타이디하게 하고 싶을 때 이 다른 길이들은 중요한 의미를 갖습니다; 그들은 각 유형의 구성요소들이 *다른 종류* 의 정보를 소통함을 상징합니다.

- `cluster` (`r nrow(points)` 개의 값들) 는 각 *점* 에 관한 정보가 있습니다
- `centers`, `withinss`, `size` (3 values) 는 각 *클러스터* 에 관한 정보가 있습니다
- `totss`, `tot.withinss`, `betweenss`, `iter` (1 값) 에는 *full clustering* 에 관한 정보가 있습니다

이 것들 중 어떤 것을 추출하고 싶을까요? 정답은 없습니다; 분석가는 각각에 관심이 있을 수 있습니다. 이것들은 완전히 다른 정보 (이것들을 결합하는 직관적인 방법이 없다는 것은 말할 필요도 없음) 를 제공하기 때문에, 추출하기 위해서 구분된 함수들이 사용됩니다. `augment` 는 point 분류를 원 데이터셋에 추가합니다:

```{r}
augment(kclust, points)
```

`tidy()` 함수는 클러스터별 수준기반으로 요약합니다:

```{r}
tidy(kclust)
```

그리고 늘 그렇듯, `glance()` 함수는 단일행 요약을 추출합니다:

```{r}
glance(kclust)
```

## 탐색적 클러스터링

이러한 요약값들이 유용하지만, 이들은 데이터셋에서 직접 추출하기 어렵지 않습니다. While these summaries are useful, they would not have been too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like [dplyr](https://dplyr.tidyverse.org/).

Let's say we want to explore the effect of different choices of `k`, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of `k`, then create columns containing the tidied, glanced and augmented data:

```{r}
kclusts <- 
  tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(points, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )

kclusts
```

We can turn these into three separate data sets each representing a different type of data: using `tidy()`, using `augment()`, and using `glance()`. Each of these goes into a separate data set as they represent different types of data.

```{r}
clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented))

clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))
```

Now we can plot the original points using the data from `augment()`, with each point colored according to the predicted cluster.

```{r, fig.width = 7, fig.height = 7}
p1 <- 
  ggplot(assignments, aes(x = x1, y = x2)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k)
p1
```

적절한 클러스터 개수 (3)와 `k` 가 너무 놓거나 낮을 때 k-means 알고리듬이 어떻게 작동하는지에 대해 좋은 감을 잠았습니다. `tidy()` 의 데이터를 이용하여 클러스터 중심들을 추가할 수 있습니다:

```{r}
p2 <- p1 + geom_point(data = clusters, size = 10, shape = "x")
p2
```

`glance()` 의 데이터는 다르지만 동등하게 중요한 목적을 만족시킵니다; `k` 값에 따른 요약 통계량 트렌드를 보여줍니다. `tot.withiness` 열에 저장된 within sum of squares 이 특별히 중요합니다.

```{r}
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()
```

이는 클러스터 내 분산을 나타냅니다. `k` 가 증가할 수록 감소하지만, `k = 3` 주위에서 꺾임 (혹은 "팔꿈치(elbow)") 이 보입니다. 이 꺽임은 3차 이후의 추가 클러스터들이 거의 소용이 없음을 가리킵니다. (수학적으로 엄밀한 해석과 이 방법의 구현에 관해서는 [여기](https://web.stanford.edu/~hastie/Papers/gap.pdf) 를 살펴보세요). 따라서, broom 이 제공하는 타이디하게 하는 세가지 방법 모두 클러스터링 결과를 요약하는데 유용합니다.

## Session information

```{r si, echo = FALSE}
small_session(pkgs)
```

