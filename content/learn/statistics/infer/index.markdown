---
title: "ë¦¬ìƒ˜í”Œë§ê³¼ íƒ€ì´ë””í•œ ë°ì´í„°ë¥¼ ì´ìš©í•œ ê°€ì„¤ê²€ì •"
tags: [infer]
categories: [statistical analysis]
type: learn-subsection
weight: 4
description: | 
  ìœ ì—°í•œ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ í†µê³„ì¶”ë¡ ì„ ìœ„í•œ ê°€ì„¤ê²€ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
---





## ë“¤ì–´ê°€ê¸°

ì´ ìž¥ì€ tidymodels íŒ¨í‚¤ì§€ë§Œ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.

tidymodels íŒ¨í‚¤ì§€ [infer](https://tidymodels.github.io/infer/)ëŠ” `tidyverse` ë””ìžì¸ í”„ë ˆìž„ì›Œí¬ì™€ ì¼ê´€ì„±ì„ ë³´ì´ëŠ” í†µê³„ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” í‘œí˜„ë ¥ ì¢‹ì€ ë¬¸ë²•ì„ êµ¬í˜„í•˜ëŠ” ë°ì— ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ íŒ¨í‚¤ì§€ëŠ” íŠ¹ì • í†µê³„ ê²€ì •ì„ ì œê³µí•˜ì§€ ì•Šê³ , ì¼ë°˜ì ì€ ê°€ì„¤ ê²€ì •ì´ ê³µìœ í•˜ëŠ” ì›ì¹™ì„ 4 ê°œì˜ ë©”ì¸ ë™ì‚¬ (í•¨ìˆ˜) ì„¸íŠ¸ë¡œ ì¢…í•©í•©ë‹ˆë‹¤ ì¶œë ¥ë¬¼ë¡œ ë¶€í„° ì •ë³´ë¥¼ ì‹œê°í™”í•˜ê³  ì¶”ì¶œí•˜ëŠ” ë„êµ¬ë“¤ì„ ìž¥ì°©í•˜ì˜€ìŠµë‹ˆë‹¤.

ìš°ë¦¬ê°€ ì–´ë–¤ ê°€ì„¤ ê²€ì •ì„ í•˜ë˜ì§€ì™€ ìƒê´€ ì—†ì´, ê°™ì€ ì¢…ë¥˜ì˜ ì§ˆë¬¸ì„ í•  ê²ƒìž…ë‹ˆë‹¤.

>ìš°ë¦¬ê°€ ê´€ì¸¡í•œ ë°ì´í„°ì—ì„œì˜ íš¨ê³¼ë‚˜ ì°¨ì´ê°€ ì‹¤ì œì¸ê°€, ì•„ë‹ˆë©´ ë‹¨ìˆœížˆ ìš°ì—°ì¸ê°€? 

ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´, ê´€ì¸¡ëœ ë°ì´í„°ëŠ” "ì•„ë¬´ê²ƒë„ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”" ì„¸ê³„ (ì¦‰, ê´€ì¸¡ëœ íš¨ê³¼ëŠ” ë‹¨ìˆœížˆ ìš°ì—°ì— ì˜í•œ ê²ƒ) ã…‡ì„œ ì™”ë‹¤ê³  ê°€ì •í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œìž‘í•˜ê³ , ì´ ê°€ì •ì„ ìš°ë¦¬ **ê·€ë¬´ê°€ì„¤(null hypothesis)** ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. (ì‹¤ì œë¡œ ê·€ë¬´ê°€ì„¤ì„ ë¯¿ëŠ” ê²ƒì€ ì „í˜€ ì•„ë‹™ë‹ˆë‹¤; ê·€ë¬´ê°€ì„¤ê³¼ ë°˜ëŒ€ì¸ **ëŒ€ë¦½ê°€ì„¤(alternative hypothesis)**ì€ ê´€ì¸¡ë°ì´í„°ì— ìžˆëŠ” íš¨ê³¼ê°€ "ë­”ê°€ê°€ ìžˆëŠ”" ì‚¬ì‹¤ì— ë¹„ë¡¯ë˜ì—ˆë‹¤ëŠ” ê²ƒìž…ë‹ˆë‹¤.) ìš°ë¦¬ëŠ” ë°ì´í„°ì—ì„œ ê´€ì¸¡ëœ íš¨ê³¼ë¥¼ ê¸°ìˆ í•˜ëŠ” **ê²€ì •í†µê³„ëŸ‰** ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ê²€ì • í†µê³„ëŸ‰ì„ ì´ìš©í•˜ì—¬ **p-ê°’** ì„ ê³„ì‚°í•  ìˆ˜ ìžˆëŠ”ë°, ì´ëŠ” ê·€ë¬´ê°€ì„¤ì´ ì‚¬ì‹¤ì¼ ë•Œ ìš°ë¦¬ ê´€ì¸¡ë°ì´í„°ê°€ ì¼ì–´ë‚  í™•ë¥ ìž…ë‹ˆë‹¤. ë¯¸ë¦¬ ì •í•œ **ìœ ì˜ìˆ˜ì¤€** `\(\alpha\)` ì´í•˜ì´ë©´ ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ê°€ì„¤ ê²€ì •ì´ ì²˜ìŒì´ë¼ë©´ ë‹¤ìŒì„ ì‚´íŽ´ë´ì•¼í•©ë‹ˆë‹¤.

* [Section 9.2 of _Statistical Inference via Data Science_](https://moderndive.com/9-hypothesis-testing.html#understanding-ht)
* The American Statistical Association's recent [statement on p-values](https://doi.org/10.1080/00031305.2016.1154108) 

ì´ íŒ¨í‚¤ì§€ì˜ ì›Œí¬í”Œë¡œëŠ” ì´ëŸ¬í•œ ìƒê°ìœ¼ë¡œ ì„¤ê³„ë©ë‹ˆë‹¤. ë°ì´í„°ì…‹ì´ ì£¼ì–´ì§€ë©´,

+ `specify()` ëŠ” ê´€ì‹¬ìžˆëŠ” ë³€ìˆ˜ë‚˜ ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
+ `hypothesize()` ëŠ” ê·€ë¬´ ê°€ì„¤ì„ ì„ ì–¸í•©ë‹ˆë‹¤.
+ `generate()` ëŠ” ê·€ë¬´ê°€ì„¤ì„ ë°˜ì˜í•˜ëŠ” ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
+ `calculate()` ëŠ” ìƒì„±ëœ ë°ì´í„°ë¡œ ë¶€í„° í†µê³„ëŸ‰ì˜ ë¶„í¬ë¥¼ ê³„ì‚°í•˜ì—¬ ê·€ë¬´ ë¶„í¬(null distribution)ë¥¼ ë§Œë“­ë‹ˆë‹¤.

ì´ vignette ì—ì„œ, infer ì— ìžˆëŠ” `gss` ë°ì´í„°ì…‹ì„ ì´ìš©í•  ê²ƒì¸ë°, ì´ëŠ” *General Social Survey* ì˜ 11 ê°œ ë³€ìˆ˜ë¥¼ ê°€ì§„ ê´€ì¸¡ê°’ 500 ê°œì˜ ìƒ˜í”Œì„ í¬í•¨í•©ë‹ˆë‹¤.


```r
library(tidymodels) # Includes the infer package

# load in the data set
data(gss)

# take a look at its structure
dplyr::glimpse(gss)
#> Rows: 500
#> Columns: 11
#> $ year    <dbl> 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20â€¦
#> $ age     <dbl> 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56â€¦
#> $ sex     <fct> male, female, male, male, male, female, female, female, femaleâ€¦
#> $ college <fct> degree, no degree, degree, no degree, degree, no degree, no deâ€¦
#> $ partyid <fct> ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, deâ€¦
#> $ hompop  <dbl> 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,â€¦
#> $ hours   <dbl> 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40â€¦
#> $ income  <ord> $25000 or more, $20000 - 24999, $25000 or more, $25000 or moreâ€¦
#> $ class   <fct> middle class, working class, working class, working class, midâ€¦
#> $ finrela <fct> below average, below average, below average, above average, abâ€¦
#> $ weight  <dbl> 0.896, 1.083, 0.550, 1.086, 1.083, 1.086, 1.063, 0.478, 1.099,â€¦
```

ê° í–‰ì€ ê°œì¸ ì¡°ì‚¬ë‹µë³€ì¸ë°, ì„¤ë¬´ìžì— ê´€í•œ ê¸°ì´ˆ ì¸êµ¬í†µê³„í•™ì • ì •ë³´ì™€ ì¶”ê°€ì ì¸ ë³€ìˆ˜ë“¤ì´ ìžˆìŠµë‹ˆë‹¤. í¬í•¨ëœ ë³€ìˆ˜ë“¤ê³¼ ì†ŒìŠ¤ì— ê´€í•œ ì •ë³´ëŠ” `?gss` ë¡œ ì•Œì•„ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„° (ì™€ ì´ì— ê´€í•œ ìš°ë¦¬ì˜ ì˜ˆì œ) ëŠ” ë³´ì—¬ì£¼ê¸° ìœ„í•œ ëª©ì ì´ê³  ì ì ˆí•œ ê°€ì¤‘ì¹˜ê°€ ì—†ë‹¤ë©´ ì •í™•í•œ ì¶”ì •ê°’ì„ ê¼­ ì œê³µí•œë‹¤ê³  í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ì˜ˆì—ì„œ, ì´ ë°ì´í„°ì…‹ì€ ìš°ë¦¬ê°€ íƒêµ¬í•˜ê³ ìží•˜ëŠ” ëª¨ì§‘ë‹¨ì¸ ë¯¸êµ­ì„±ì¸ì§‘ë‹¨ì„ ëŒ€í‘œí•  ìˆ˜ ìžˆëŠ” ìƒ˜í”Œì´ë¼ê³  ê°€ì •í•©ì‹œë‹¤.

## ë³€ìˆ˜ ì„¤ì •

The `specify()` í•¨ìˆ˜ëŠ” ë°ì´í„°ì…‹ì—ì„œ ì–´ë–¤ ë³€ìˆ˜ì— ê´€ì‹¬ì´ ìžˆëŠ”ì§€ë¥¼ ì„¤ì •í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë§Œì•½ ì‘ë‹µìžì˜ `age` ì—ë§Œ ê´€ì‹¬ì´ ìžˆë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ ìž‘ì„±í•©ë‹ˆë‹¤:


```r
gss %>%
  specify(response = age)
#> Response: age (numeric)
#> # A tibble: 500 Ã— 1
#>      age
#>    <dbl>
#>  1    36
#>  2    34
#>  3    24
#>  4    42
#>  5    31
#>  6    32
#>  7    48
#>  8    36
#>  9    30
#> 10    33
#> # â€¦ with 490 more rows
```


í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë³´ë©´, `specify()` ì˜ ì¶œë ¥ì€ ì„¤ì •í•œ ë°ì´í„°í”„ë ˆìž„ì˜ ì—´ë“¤ì„ ì½• ì°ëŠ” ê²ƒ ì²˜ëŸ¼ ë³´ìž…ë‹ˆë‹¤. ì´ ê°ì²´ì˜ í´ëž˜ìŠ¤ë¥¼ í™•ì¸í•˜ê³  ì‹¶ìœ¼ë©´ ì–´ë–»ê²Œ í• ê¹Œìš”?


```r
gss %>%
  specify(response = age) %>%
  class()
#> [1] "infer"      "tbl_df"     "tbl"        "data.frame"
```

infer í´ëž˜ìŠ¤ëŠ” ë°ì´í„°í”„ë ˆìž„ í´ëž˜ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ëœ ê²ƒìž„ì„ ì•Œ ìˆ˜ ìžˆìŠµë‹ˆë‹¤; ì´ ìƒˆë¡œìš´ í´ëž˜ìŠ¤ëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.

ë‘ ê°œì˜ ë³€ìˆ˜ (ì˜ˆë¥¼ ë“¤ì–´ `age` ì™€ `partyid`) ì— ê´€ì‹¬ì´ ìžˆë‹¤ë©´ ì´ë“¤ì˜ ê´€ê³„ë¥¼ ë‘ ë°©ë²• ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ ì„¤ì •(`specify()`)í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:


```r
# as a formula
gss %>%
  specify(age ~ partyid)
#> Response: age (numeric)
#> Explanatory: partyid (factor)
#> # A tibble: 500 Ã— 2
#>      age partyid
#>    <dbl> <fct>  
#>  1    36 ind    
#>  2    34 rep    
#>  3    24 ind    
#>  4    42 ind    
#>  5    31 rep    
#>  6    32 rep    
#>  7    48 dem    
#>  8    36 ind    
#>  9    30 rep    
#> 10    33 dem    
#> # â€¦ with 490 more rows

# with the named arguments
gss %>%
  specify(response = age, explanatory = partyid)
#> Response: age (numeric)
#> Explanatory: partyid (factor)
#> # A tibble: 500 Ã— 2
#>      age partyid
#>    <dbl> <fct>  
#>  1    36 ind    
#>  2    34 rep    
#>  3    24 ind    
#>  4    42 ind    
#>  5    31 rep    
#>  6    32 rep    
#>  7    48 dem    
#>  8    36 ind    
#>  9    30 rep    
#> 10    33 dem    
#> # â€¦ with 490 more rows
```

ë¹„ìœ¨ì´ë‚˜ ë¹„ìœ¨ì˜ ì°¨ì— ê´€í•œ ì¶”ë¡ ì„ í•˜ê³  ìžˆë‹¤ë©´, `success` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `response` ë³€ìˆ˜ì˜ ì–´ë–¤ ìˆ˜ì¤€ì´ ì„±ê³µ(success) ì¸ì§€ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëŒ€í•™ í•™ìœ„ê°€ ìžˆëŠ” ëª¨ì§‘ë‹¨ì˜ ë¹„ìœ¨ì— ê´€ì‹¬ì´ ìžˆë‹¤ë©´, ë‹¤ìŒ ì½”ë“œë¥¼ ì´ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤: 


```r
# specifying for inference on proportions
gss %>%
  specify(response = college, success = "degree")
#> Response: college (factor)
#> # A tibble: 500 Ã— 1
#>    college  
#>    <fct>    
#>  1 degree   
#>  2 no degree
#>  3 degree   
#>  4 no degree
#>  5 degree   
#>  6 no degree
#>  7 no degree
#>  8 degree   
#>  9 degree   
#> 10 no degree
#> # â€¦ with 490 more rows
```

## ê°€ì„¤ ì„ ì–¸

ì¶”ë¡  íŒŒì´í”„ë¼ì¸ì—ì„œ ë‹¤ìŒ ê³¼ì •ì€ ì¢…ì¢… `hypothesize()` ì„ ì´ìš©í•œ ê·€ë¬´ê°€ì„¤ ì„ ì–¸ìž…ë‹ˆë‹¤. ì²«ë²ˆì§¸ ë‹¨ê³„ëŠ” `null` "independence" ë‚˜ "point" ì¤‘ í•˜ë‚˜ë¥¼ `null` ì¸ìˆ˜ì— ì œê³µí•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤. ê·€ë¬´ê°€ì„¤ì´ ë‘ ë³€ìˆ˜ê°„ ë…ë¦½ì„ ê°€ì •í•œë‹¤ë©´, `hypothesize()` ì— ì œê³µí•´ì•¼í•˜ëŠ” ê²ƒì€ ì´ê²ƒìœ¼ë¡œ ì¡±í•©ë‹ˆë‹¤:


```r
gss %>%
  specify(college ~ partyid, success = "degree") %>%
  hypothesize(null = "independence")
#> Response: college (factor)
#> Explanatory: partyid (factor)
#> Null Hypothesis: independence
#> # A tibble: 500 Ã— 2
#>    college   partyid
#>    <fct>     <fct>  
#>  1 degree    ind    
#>  2 no degree rep    
#>  3 degree    ind    
#>  4 no degree ind    
#>  5 degree    rep    
#>  6 no degree rep    
#>  7 no degree dem    
#>  8 degree    ind    
#>  9 degree    rep    
#> 10 no degree dem    
#> # â€¦ with 490 more rows
```

ì  ì¶”ì •ì— ê´€í•œ ì¶”ë¡ ì„ í•˜ê³  ìžˆë‹¤ë©´, `p` (the true proportion of successes, between 0 and 1), `mu` (the true mean), `med` (the true median), `sigma` (the true standard deviation) ì¤‘ í•˜ë‚˜ë„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê·€ë¬´ê°€ì„¤ì´ ëª¨ì§‘ë‹¨ì—ì„œ ì£¼ë‹¹ê·¼ë¬´ì‹œê°„ì´ 40 ì´ë‹¤ ì´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìž‘ì„±í•©ë‹ˆë‹¤:


```r
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40)
#> Response: hours (numeric)
#> Null Hypothesis: point
#> # A tibble: 500 Ã— 1
#>    hours
#>    <dbl>
#>  1    50
#>  2    31
#>  3    40
#>  4    40
#>  5    40
#>  6    53
#>  7    32
#>  8    20
#>  9    40
#> 10    40
#> # â€¦ with 490 more rows
```

í”„ë¡ íŠ¸ì—”ë“œì—ì„œ `hypothesize()` ì¶œë ¥ ë°ì´í„°í”„ë ˆìž„ì€ `specify()` ì—ì„œ ë‚˜ì™”ì„ ë•Œì™€ ê±°ì˜ ê°™ì€ ê²ƒ ê°™ì§€ë§Œ, infer ëŠ” ì§€ê¸ˆ ë‹¹ì‹ ì˜ ê·€ë¬´ê°€ì„¤ì„ "ì•Œê³ ìžˆìŠµë‹ˆë‹¤".

## ë¶„í¬ ìƒì„±í•˜ê¸°

Once we've asserted our null hypothesis using `hypothesize()`, we can construct a null distribution based on this hypothesis. We can do this using one of several methods, supplied in the `type` argument:

* `bootstrap`: A bootstrap sample will be drawn for each replicate, where a sample of size equal to the input sample size is drawn (with replacement) from the input sample data.  
* `permute`: For each replicate, each input value will be randomly reassigned (without replacement) to a new output value in the sample.  
* `simulate`: A value will be sampled from a theoretical distribution with parameters specified in `hypothesize()` for each replicate. (This option is currently only applicable for testing point estimates.)  

Continuing on with our example above, about the average number of hours worked a week, we might write:


```r
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 5000, type = "bootstrap")
#> Response: hours (numeric)
#> Null Hypothesis: point
#> # A tibble: 2,500,000 Ã— 2
#> # Groups:   replicate [5,000]
#>    replicate hours
#>        <int> <dbl>
#>  1         1 18.6 
#>  2         1 18.6 
#>  3         1 38.6 
#>  4         1 33.6 
#>  5         1 28.6 
#>  6         1 38.6 
#>  7         1 38.6 
#>  8         1  8.62
#>  9         1 54.6 
#> 10         1 38.6 
#> # â€¦ with 2,499,990 more rows
```

ìœ„ ì˜ˆì—ì„œ, ê·€ë¬´ ê°€ì„¤ì„ í˜•ì„±í•˜ê¸° ìœ„í•´ 5000 ê°œì˜ ë¶€íŠ¸ìŠ¤íŠ¸ëž© ìƒ˜í”Œì„ ì·¨í•©ë‹ˆë‹¤.


To generate a null distribution for the independence of two variables, we could also randomly reshuffle the pairings of explanatory and response variables to break any existing association. For instance, to generate 5000 replicates that can be used to create a null distribution under the assumption that political party affiliation is not affected by age:


```r
gss %>%
  specify(partyid ~ age) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 5000, type = "permute")
#> Response: partyid (factor)
#> Explanatory: age (numeric)
#> Null Hypothesis: independence
#> # A tibble: 2,500,000 Ã— 3
#> # Groups:   replicate [5,000]
#>    partyid   age replicate
#>    <fct>   <dbl>     <int>
#>  1 ind        36         1
#>  2 ind        34         1
#>  3 ind        24         1
#>  4 ind        42         1
#>  5 ind        31         1
#>  6 ind        32         1
#>  7 dem        48         1
#>  8 ind        36         1
#>  9 other      30         1
#> 10 dem        33         1
#> # â€¦ with 2,499,990 more rows
```

## Calculate statistics

Depending on whether you're carrying out computation-based inference or theory-based inference, you will either supply `calculate()` with the output of `generate()` or `hypothesize()`, respectively. The function, for one, takes in a `stat` argument, which is currently one of `"mean"`, `"median"`, `"sum"`, `"sd"`, `"prop"`, `"count"`, `"diff in means"`, `"diff in medians"`, `"diff in props"`, `"Chisq"`, `"F"`, `"t"`, `"z"`, `"slope"`, or `"correlation"`. For example, continuing our example above to calculate the null distribution of mean hours worked per week:


```r
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 5000, type = "bootstrap") %>%
  calculate(stat = "mean")
#> Response: hours (numeric)
#> Null Hypothesis: point
#> # A tibble: 5,000 Ã— 2
#>    replicate  stat
#>        <int> <dbl>
#>  1         1  39.8
#>  2         2  40.2
#>  3         3  40.7
#>  4         4  39.3
#>  5         5  40.8
#>  6         6  40.6
#>  7         7  39.5
#>  8         8  39.4
#>  9         9  40.2
#> 10        10  41.8
#> # â€¦ with 4,990 more rows
```

The output of `calculate()` here shows us the sample statistic (in this case, the mean) for each of our 1000 replicates. If you're carrying out inference on differences in means, medians, or proportions, or `\(t\)` and `\(z\)` statistics, you will need to supply an `order` argument, giving the order in which the explanatory variables should be subtracted. For instance, to find the difference in mean age of those that have a college degree and those that don't, we might write:


```r
gss %>%
  specify(age ~ college) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 5000, type = "permute") %>%
  calculate("diff in means", order = c("degree", "no degree"))
#> Response: age (numeric)
#> Explanatory: college (factor)
#> Null Hypothesis: independence
#> # A tibble: 5,000 Ã— 2
#>    replicate    stat
#>        <int>   <dbl>
#>  1         1  2.51  
#>  2         2 -2.24  
#>  3         3  2.26  
#>  4         4  0.897 
#>  5         5  2.98  
#>  6         6 -0.0113
#>  7         7 -0.144 
#>  8         8  1.71  
#>  9         9  2.42  
#> 10        10  0.0504
#> # â€¦ with 4,990 more rows
```

## Other utilities

The infer package also offers several utilities to extract meaning out of summary statistics and null distributions; the package provides functions to visualize where a statistic is relative to a distribution (with `visualize()`), calculate p-values (with `get_p_value()`), and calculate confidence intervals (with `get_confidence_interval()`).

To illustrate, we'll go back to the example of determining whether the mean number of hours worked per week is 40 hours.


```r
# find the point estimate
point_estimate <- gss %>%
  specify(response = hours) %>%
  calculate(stat = "mean")

# generate a null distribution
null_dist <- gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 5000, type = "bootstrap") %>%
  calculate(stat = "mean")
```

(Notice the warning: `Removed 1244 rows containing missing values.` This would be worth noting if you were actually carrying out this hypothesis test.)

Our point estimate 41.382 seems *pretty* close to 40, but a little bit different. We might wonder if this difference is just due to random chance, or if the mean number of hours worked per week in the population really isn't 40.

We could initially just visualize the null distribution.


```r
null_dist %>%
  visualize()
```

<img src="figs/visualize-1.svg" width="672" />

Where does our sample's observed statistic lie on this distribution? We can use the `obs_stat` argument to specify this.


```r
null_dist %>%
  visualize() +
  shade_p_value(obs_stat = point_estimate, direction = "two_sided")
```

<img src="figs/visualize2-1.svg" width="672" />

Notice that infer has also shaded the regions of the null distribution that are as (or more) extreme than our observed statistic. (Also, note that we now use the `+` operator to apply the `shade_p_value()` function. This is because `visualize()` outputs a plot object from ggplot2 instead of a dataframe, and the `+` operator is needed to add the p-value layer to the plot object.) The red bar looks like it's slightly far out on the right tail of the null distribution, so observing a sample mean of 41.382 hours would be somewhat unlikely if the mean was actually 40 hours. How unlikely, though?


```r
# get a two-tailed p-value
p_value <- null_dist %>%
  get_p_value(obs_stat = point_estimate, direction = "two_sided")

p_value
#> # A tibble: 1 Ã— 1
#>   p_value
#>     <dbl>
#> 1  0.0368
```

It looks like the p-value is 0.037, which is pretty small---if the true mean number of hours worked per week was actually 40, the probability of our sample mean being this far (1.382 hours) from 40 would be 0.037. This may or may not be statistically significantly different, depending on the significance level `\(\alpha\)` you decided on *before* you ran this analysis. If you had set `\(\alpha = .05\)`, then this difference would be statistically significant, but if you had set `\(\alpha = .01\)`, then it would not be.

To get a confidence interval around our estimate, we can write:


```r
# start with the null distribution
null_dist %>%
  # calculate the confidence interval around the point estimate
  get_confidence_interval(point_estimate = point_estimate,
                          # at the 95% confidence level
                          level = .95,
                          # using the standard error
                          type = "se")
#> # A tibble: 1 Ã— 2
#>   lower_ci upper_ci
#>      <dbl>    <dbl>
#> 1     40.1     42.7
```

As you can see, 40 hours per week is not contained in this interval, which aligns with our previous conclusion that this finding is significant at the confidence level `\(\alpha = .05\)`.

## Theoretical methods

The infer package also provides functionality to use theoretical methods for `"Chisq"`, `"F"` and `"t"` test statistics. 

Generally, to find a null distribution using theory-based methods, use the same code that you would use to find the null distribution using randomization-based methods, but skip the `generate()` step. For example, if we wanted to find a null distribution for the relationship between age (`age`) and party identification (`partyid`) using randomization, we could write:


```r
null_f_distn <- gss %>%
   specify(age ~ partyid) %>%
   hypothesize(null = "independence") %>%
   generate(reps = 5000, type = "permute") %>%
   calculate(stat = "F")
```

To find the null distribution using theory-based methods, instead, skip the `generate()` step entirely:


```r
null_f_distn_theoretical <- gss %>%
   specify(age ~ partyid) %>%
   hypothesize(null = "independence") %>%
   calculate(stat = "F")
```

We'll calculate the observed statistic to make use of in the following visualizations; this procedure is the same, regardless of the methods used to find the null distribution.


```r
F_hat <- gss %>% 
  specify(age ~ partyid) %>%
  calculate(stat = "F")
```

Now, instead of just piping the null distribution into `visualize()`, as we would do if we wanted to visualize the randomization-based null distribution, we also need to provide `method = "theoretical"` to `visualize()`.


```r
visualize(null_f_distn_theoretical, method = "theoretical") +
  shade_p_value(obs_stat = F_hat, direction = "greater")
```

<img src="figs/unnamed-chunk-4-1.svg" width="672" />

To get a sense of how the theory-based and randomization-based null distributions relate, we can pipe the randomization-based null distribution into `visualize()` and also specify `method = "both"`


```r
visualize(null_f_distn, method = "both") +
  shade_p_value(obs_stat = F_hat, direction = "greater")
```

<img src="figs/unnamed-chunk-5-1.svg" width="672" />

That's it! This vignette covers most all of the key functionality of infer. See `help(package = "infer")` for a full list of functions and vignettes.


## Session information


```
#> â”€ Session info  ðŸ‘£  ðŸ‘·ðŸ¿  ðŸ‘¨ðŸ¼â€ðŸ¦³   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  hash: footprints, construction worker: dark skin tone, man: medium-light skin tone, white hair
#> 
#>  setting  value
#>  version  R version 4.1.1 (2021-08-10)
#>  os       macOS Big Sur 10.16
#>  system   x86_64, darwin17.0
#>  ui       X11
#>  language (EN)
#>  collate  en_US.UTF-8
#>  ctype    en_US.UTF-8
#>  tz       Asia/Seoul
#>  date     2022-01-12
#>  pandoc   2.11.4 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown)
#> 
#> â”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#>  package    * version date (UTC) lib source
#>  broom      * 0.7.11  2022-01-03 [1] CRAN (R 4.1.2)
#>  dials      * 0.0.10  2021-09-10 [1] CRAN (R 4.1.0)
#>  dplyr      * 1.0.7   2021-06-18 [1] CRAN (R 4.1.0)
#>  ggplot2    * 3.3.5   2021-06-25 [1] CRAN (R 4.1.0)
#>  infer      * 1.0.0   2021-08-13 [1] CRAN (R 4.1.0)
#>  parsnip    * 0.1.7   2021-07-21 [1] CRAN (R 4.1.0)
#>  purrr      * 0.3.4   2020-04-17 [1] CRAN (R 4.1.0)
#>  recipes    * 0.1.17  2021-09-27 [1] CRAN (R 4.1.0)
#>  rlang        0.4.12  2021-10-18 [1] CRAN (R 4.1.0)
#>  rsample    * 0.1.1   2021-11-08 [1] CRAN (R 4.1.0)
#>  tibble     * 3.1.6   2021-11-07 [1] CRAN (R 4.1.0)
#>  tidymodels * 0.1.4   2021-10-01 [1] CRAN (R 4.1.0)
#>  tune       * 0.1.6   2021-07-21 [1] CRAN (R 4.1.0)
#>  workflows  * 0.2.4   2021-10-12 [1] CRAN (R 4.1.0)
#>  yardstick  * 0.0.9   2021-11-22 [1] CRAN (R 4.1.0)
#> 
#>  [1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library
#> 
#> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```
 
